import os
import re
import json
import logging
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timezone, timedelta
from urllib.parse import urlparse, urlunparse, parse_qsl

import requests
import streamlit as st
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from difflib import SequenceMatcher
from email.utils import parsedate_to_datetime

# =========================
# ë¡œê¹…
# =========================
logging.basicConfig(
    format="%(asctime)s | %(levelname)s | %(message)s",
    level=logging.INFO,
)

# =========================
# ê²½ë¡œ/ìƒìˆ˜
# =========================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STORAGE_DIR = os.environ.get("STORAGE_DIR", os.path.join(BASE_DIR, ".pe_news_state"))
os.makedirs(STORAGE_DIR, exist_ok=True)
SENT_DB_PATH = os.path.join(STORAGE_DIR, "sent_urls.json")

CONFIG_PATH = os.environ.get("CONFIG_PATH", os.path.join(BASE_DIR, "config.json"))

DEFAULT_CONFIG = {
    "KEYWORDS": [],
    "KEYWORD_ALIASES": {},
    "FIRM_WATCHLIST": [],
    "ALLOW_DOMAINS": [],
    "BLOCK_DOMAINS": ["sports.naver.com", "m.sports.naver.com"],
    "INCLUDE_TITLE_KEYWORDS": [],
    "EXCLUDE_TITLE_KEYWORDS": [],
    "DOMAIN_WEIGHTS": {
        "thebell.co.kr": 3.0,
        "investchosun.com": 3.0,
        "dealsite.co.kr": 2.5,
        "news.naver.com": 1.0,
        "sports.naver.com": -5.0,
        "m.sports.naver.com": -5.0
    },
    "RECENCY_HOURS": 48,
    "ONLY_WORKING_HOURS": True,
    "TELEGRAM_DISABLE_PREVIEW": True,
    "MAX_PER_KEYWORD": 10,
    "PAGE_SIZE": 30,
    "CREDENTIALS": {}
}

# =========================
# ê³µí†µ ìœ í‹¸
# =========================
def read_json(path: str, default):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default

def write_json(path: str, data):
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)

def load_config(path: str = CONFIG_PATH) -> Dict:
    cfg = DEFAULT_CONFIG.copy()
    file_cfg = read_json(path, default=None)
    if file_cfg is None:
        logging.warning("config.json not found. Using DEFAULT_CONFIG. path=%s", path)
        return cfg
    cfg.update(file_cfg)
    return cfg

CONFIG = load_config()

def get_secret(key: str, default: str = "") -> str:
    """í™˜ê²½ë³€ìˆ˜ â†’ config.json(CREDENTIALS) ìˆœìœ¼ë¡œ ì¡°íšŒ."""
    env_val = os.environ.get(key)
    if env_val:
        return env_val
    creds = CONFIG.get("CREDENTIALS", {})
    return creds.get(key, default)

def now_utc() -> datetime:
    return datetime.utcnow().replace(tzinfo=timezone.utc)

def html_unescape(s: str) -> str:
    return (s or "").replace("&quot;", '"').replace("&amp;", "&").replace("&apos;", "'")\
                    .replace("&lt;", "<").replace("&gt;", ">")

# URL ì •ê·œí™”(ì¶”ì  íŒŒë¼ë¯¸í„° ì œê±°)
TRACKING_PARAMS = {
    "utm_source","utm_medium","utm_campaign","utm_term","utm_content",
    "inflow","sid","oid","aid","mode","ref","feature","from"
}
def normalize_url(u: str) -> str:
    u = (u or "").strip().replace("http://", "https://")
    try:
        p = urlparse(u)
        q = [(k, v) for k, v in parse_qsl(p.query, keep_blank_values=True)
             if k.lower() not in TRACKING_PARAMS]
        return urlunparse((
            p.scheme,
            p.netloc.lower(),
            p.path,
            p.params,
            "&".join([f"{k}={v}" for k, v in q]),
            ""  # fragment ì œê±°
        ))
    except Exception:
        return u

def domain_of(url: str) -> str:
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""

def iso_to_dt(iso_str: str) -> datetime:
    try:
        return datetime.fromisoformat(iso_str.replace("Z", "+00:00"))
    except Exception:
        return now_utc()

def hours_ago(iso_str: str) -> float:
    return (now_utc() - iso_to_dt(iso_str)).total_seconds() / 3600.0

# =========================
# ë°ì´í„° ëª¨ë¸/ê³µê¸‰ì
# =========================
@dataclass
class Article:
    title: str
    url: str
    source: str
    published_at: str  # ISO 8601
    description: Optional[str] = None

class NewsProvider:
    def fetch(self, query: str, page_size: int = 20) -> List[Article]:
        raise NotImplementedError

class NewsAPIProvider(NewsProvider):
    def __init__(self, api_key: str):
        self.api_key = api_key
    def fetch(self, query: str, page_size: int = 20) -> List[Article]:
        url = "https://newsapi.org/v2/everything"
        params = {"q": query, "language": "ko", "pageSize": page_size, "sortBy": "publishedAt"}
        r = requests.get(url, params=params, headers={"X-Api-Key": self.api_key}, timeout=20)
        r.raise_for_status()
        data = r.json()
        arts = []
        for a in data.get("articles", []):
            arts.append(Article(
                title=a.get("title") or "",
                url=a.get("url") or "",
                source=(a.get("source") or {}).get("name") or "NewsAPI",
                published_at=(a.get("publishedAt") or now_utc().isoformat()),
                description=a.get("description") or "",
            ))
        return arts

class NaverNewsProvider(NewsProvider):
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
    def fetch(self, query: str, page_size: int = 20) -> List[Article]:
        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {"X-Naver-Client-Id": self.client_id, "X-Naver-Client-Secret": self.client_secret}
        params = {"query": query, "display": min(page_size, 100), "start": 1, "sort": "date"}
        r = requests.get(url, params=params, headers=headers, timeout=20)
        r.raise_for_status()
        data = r.json()
        arts = []
        for it in data.get("items", []):
            pub = it.get("pubDate") or ""
            try:
                published = parsedate_to_datetime(pub).astimezone(timezone.utc).isoformat()
            except Exception:
                published = now_utc().isoformat()
            arts.append(Article(
                title=it.get("title") or "",
                url=(it.get("link") or it.get("originallink") or ""),
                source="Naver",
                published_at=published,
                description=it.get("description") or "",
            ))
        return arts

# =========================
# ë­í‚¹/í•„í„°/ì¤‘ë³µì œê±°
# =========================
WORD_RE = re.compile(r"[ê°€-í£A-Za-z0-9]+")

def title_key(title: str) -> str:
    t = html_unescape(title).lower()
    tokens = WORD_RE.findall(t)
    tokens = [w for w in tokens if len(w) >= 2 and not w.isdigit()]
    return " ".join(tokens[:30])

def is_similar(t1: str, t2: str, threshold: float = 0.86) -> bool:
    return SequenceMatcher(None, title_key(t1), title_key(t2)).ratio() >= threshold

def dedup_by_title(articles: List[Article], score_fn) -> List[Article]:
    reps: List[Article] = []
    for a in articles:
        dup = False
        for i, r in enumerate(reps):
            if is_similar(a.title, r.title):
                if score_fn(a) > score_fn(r):
                    reps[i] = a
                dup = True
                break
        if not dup:
            reps.append(a)
    return reps

def should_keep(a: Article,
                allow_domains: List[str],
                block_domains: List[str],
                must_include_terms: List[str],
                must_exclude_terms: List[str],
                recency_hours: int) -> bool:
    d = domain_of(a.url)
    if d in block_domains:
        return False
    if allow_domains and d not in allow_domains:
        return False
    if recency_hours > 0 and hours_ago(a.published_at) > recency_hours:
        return False
    title = html_unescape(a.title).lower()
    if must_include_terms and not any(t for t in must_include_terms if t in title):
        return False
    if any(t for t in must_exclude_terms if t in title):
        return False
    return True

# =========================
# Telegram
# =========================
def send_telegram_message(bot_token: str, chat_id: str, text: str, disable_web_page_preview=True):
    r = requests.post(
        f"https://api.telegram.org/bot{bot_token}/sendMessage",
        data={
            "chat_id": chat_id,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": disable_web_page_preview
        },
        timeout=20,
    )
    if r.status_code != 200:
        logging.error("Telegram send failed: %s", r.text)
    r.raise_for_status()
    return r.json()

def compose_message(keyword: str, articles: List[Article]) -> str:
    lines = [f"ğŸ“Œ <b>PE ë™í–¥ ë‰´ìŠ¤ ({keyword})</b>"]
    for a in articles:
        dt = a.published_at[:16].replace("T", " ")
        lines.append(
            f"â€¢ <a href=\"{a.url}\">{html_unescape(a.title)}</a> â€” <i>{a.source}</i> ({dt})"
        )
    return "\n".join(lines)[:4000]  # í…”ë ˆê·¸ë¨ 4096ì ì œí•œ ê°€ë“œ

# =========================
# Streamlit ìƒíƒœ
# =========================
if "scheduler" not in st.session_state:
    st.session_state.scheduler = None
if "is_running" not in st.session_state:
    st.session_state.is_running = False
if "last_run" not in st.session_state:
    st.session_state.last_run = None

st.set_page_config(page_title="PE ë™í–¥ ë‰´ìŠ¤ â†’ Telegram", page_icon="ğŸ“¨", layout="wide")
st.title("ğŸ“¨ PE ë™í–¥ ë‰´ìŠ¤ â†’ Telegram ìë™ ì „ì†¡")
st.caption("í‚¤ì›Œë“œëŠ” ì „ë¶€ config.jsonì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤.")

# =========================
# Sidebar
# =========================
with st.sidebar:
    st.subheader("ìê²©ì¦ëª… / ì„¤ì •")
    st.caption(f"CONFIG ê²½ë¡œ: {CONFIG_PATH} / ì¡´ì¬: {os.path.isfile(CONFIG_PATH)}")

    newsapi_key = st.text_input("NewsAPI Key (ì„ íƒ)", value=get_secret("NEWSAPI_KEY"), type="password")
    naver_client_id = st.text_input("Naver Client ID (ì„ íƒ)", value=get_secret("NAVER_CLIENT_ID"), type="password")
    naver_client_secret = st.text_input("Naver Client Secret (ì„ íƒ)", value=get_secret("NAVER_CLIENT_SECRET"), type="password")

    telegram_bot_token = st.text_input("Telegram Bot Token", value=get_secret("TELEGRAM_BOT_TOKEN"), type="password")
    telegram_chat_id   = st.text_input("Telegram Chat ID (ë³¸ì¸/ê·¸ë£¹)", value=get_secret("TELEGRAM_CHAT_ID"))

    st.divider()
    st.subheader("config.json")
    if st.button("êµ¬ì„± ë¦¬ë¡œë“œ"):
        global CONFIG
        CONFIG = load_config()
        st.success("config.jsonì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

    st.text("KEYWORDS (ì½ê¸°ì „ìš©)")
    st.code("\n".join(CONFIG.get("KEYWORDS", [])) or "(none)", language="text")

    def csv(v): return ", ".join([x for x in v if isinstance(x, str)])
    page_size        = st.number_input("í˜ì´ì§€ë‹¹ ìˆ˜ì§‘ ìˆ˜", min_value=5, max_value=100, value=int(CONFIG.get("PAGE_SIZE", 30)), step=5)
    max_per_keyword  = st.number_input("ì „ì†¡ ê±´ìˆ˜ ì œí•œ(í‚¤ì›Œë“œë³„)", min_value=1, max_value=50, value=int(CONFIG.get("MAX_PER_KEYWORD", 10)), step=1)
    interval_min     = st.number_input("ì „ì†¡ ì£¼ê¸°(ë¶„)", min_value=5, max_value=720, value=60, step=5)
    recency_hours    = st.number_input("ì‹ ì„ ë„(ìµœê·¼ Nì‹œê°„)", min_value=0, max_value=168, value=int(CONFIG.get("RECENCY_HOURS", 48)), step=1)
    only_working     = st.checkbox("ì—…ë¬´ì‹œê°„(08â€“20 KST) ë‚´ ì „ì†¡", value=bool(CONFIG.get("ONLY_WORKING_HOURS", True)))
    disable_preview  = st.checkbox("ë§í¬ í”„ë¦¬ë·° ë¹„í™œì„±í™”", value=bool(CONFIG.get("TELEGRAM_DISABLE_PREVIEW", True)))
    test_mode        = st.checkbox("í…ŒìŠ¤íŠ¸ ëª¨ë“œ(ì „ì†¡ ìƒëµ)", value=False)

    allow_domains_txt = st.text_input("í—ˆìš© ë„ë©”ì¸(ì‰¼í‘œ; ë¹„ìš°ë©´ ì „ì²´ í—ˆìš©)", value=csv(CONFIG.get("ALLOW_DOMAINS", [])))
    block_domains_txt = st.text_input("ì°¨ë‹¨ ë„ë©”ì¸(ì‰¼í‘œ)", value=csv(CONFIG.get("BLOCK_DOMAINS", [])))
    include_terms_txt = st.text_input("í¬í•¨ í‚¤ì›Œë“œ(ì œëª©)", value=csv(CONFIG.get("INCLUDE_TITLE_KEYWORDS", [])))
    exclude_terms_txt = st.text_input("ì œëª© ì œì™¸ í‚¤ì›Œë“œ", value=csv(CONFIG.get("EXCLUDE_TITLE_KEYWORDS", [])))

def csv_to_list(s: str) -> List[str]:
    return [x.strip().lower() for x in s.split(",") if x.strip()]

allow_domains = csv_to_list(allow_domains_txt)
block_domains = csv_to_list(block_domains_txt)
include_terms = csv_to_list(include_terms_txt)
exclude_terms = csv_to_list(exclude_terms_txt)

DOMAIN_WEIGHTS = CONFIG.get("DOMAIN_WEIGHTS", {})
FIRM_WATCH = [s.lower() for s in CONFIG.get("FIRM_WATCHLIST", [])]

# =========================
# ê³µê¸‰ì êµ¬ì„±
# =========================
providers: List[NewsProvider] = []
if newsapi_key:
    providers.append(NewsAPIProvider(newsapi_key))
if naver_client_id and naver_client_secret:
    providers.append(NaverNewsProvider(naver_client_id, naver_client_secret))
if not providers:
    st.warning("ìµœì†Œ í•˜ë‚˜ì˜ ë‰´ìŠ¤ ì œê³µì(NewsAPI ë˜ëŠ” Naver)ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

# =========================
# ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
# =========================
def expand_queries(kw: str) -> List[str]:
    aliases = CONFIG.get("KEYWORD_ALIASES", {}).get(kw, [])
    base = [kw] + [a for a in aliases if a != kw]
    enriched: List[str] = []
    for q in base:
        ql = q.lower()
        if ql in {"buyout", "m&a", "private equity"}:
            enriched.append(f"{q} AND (ì¸ìˆ˜ OR ë§¤ê° OR ë”œ OR PEF)")
        else:
            enriched.append(q)
    seen = set(); uniq=[]
    for q in enriched:
        if q not in seen:
            uniq.append(q); seen.add(q)
    return uniq

def fetch_articles(providers: List[NewsProvider], query: str, page_size: int) -> List[Article]:
    agg: Dict[str, Article] = {}
    for p in providers:
        try:
            for a in p.fetch(query, page_size=page_size):
                u = normalize_url(a.url)
                if u and u not in agg:
                    agg[u] = Article(a.title, u, a.source, a.published_at, a.description)
        except Exception as e:
            logging.warning("Provider fetch error (%s, %s): %s", p.__class__.__name__, query, e)
    return list(agg.values())

def score_article(a: Article) -> float:
    score = DOMAIN_WEIGHTS.get(domain_of(a.url), 0.0)
    h = max(0.0, min(48.0, hours_ago(a.published_at)))
    score += (48.0 - h) / 48.0 * 2.0  # ìµœì‹  ê°€ì 
    title = html_unescape(a.title).lower()
    if any(t for t in include_terms if t in title):
        score += 1.0
    if any(t for t in exclude_terms if t in title):
        score -= 2.0
    if any(f in title for f in FIRM_WATCH):
        score += 0.8
    if len(title) < 10:
        score -= 1.0
    return score

def do_run_once() -> Tuple[int, int]:
    if not providers:
        st.error("ì„¤ì •ëœ ë‰´ìŠ¤ ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 0
    if (not telegram_bot_token or not telegram_chat_id) and not test_mode:
        st.error("Telegram í† í°/ì±„íŒ… IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return 0, 0
    if only_working:
        kst_hour = (datetime.utcnow() + timedelta(hours=9)).hour
        if kst_hour < 8 or kst_hour >= 20:
            logging.info("Skipped by working-hours window (KST %02d)", kst_hour)
            return 0, 0

    sent_db: Dict[str, bool] = read_json(SENT_DB_PATH, default={})
    total_sent = 0
    kw_sent_cnt = 0

    keywords = CONFIG.get("KEYWORDS", [])
    for kw in [k.strip() for k in keywords if k.strip()]:
        raw: List[Article] = []
        for q in expand_queries(kw):
            raw.extend(fetch_articles(providers, q, page_size=int(CONFIG.get("PAGE_SIZE", 30))))

        url_seen: Dict[str, Article] = {}
        for a in raw:
            u = normalize_url(a.url)
            if u and u not in url_seen:
                url_seen[u] = a
        raw = list(url_seen.values())

        filt = [a for a in raw if should_keep(
            a,
            allow_domains,
            block_domains,
            include_terms,
            exclude_terms,
            int(CONFIG.get("RECENCY_HOURS", 48))
        )]

        filt = dedup_by_title(filt, score_article)

        ranked = sorted(
            filt,
            key=lambda x: (score_article(x), x.published_at),
            reverse=True
        )

        new_arts: List[Article] = []
        for a in ranked:
            if a.url in sent_db:
                continue
            new_arts.append(a)
            if len(new_arts) >= int(CONFIG.get("MAX_PER_KEYWORD", 10)):
                break

        if not new_arts:
            continue

        msg = compose_message(kw, new_arts)
        try:
            if test_mode:
                logging.info("[TEST MODE] Would send %d items for '%s'", len(new_arts), kw)
            else:
                send_telegram_message(
                    telegram_bot_token, telegram_chat_id, msg,
                    disable_web_page_preview=bool(CONFIG.get("TELEGRAM_DISABLE_PREVIEW", True))
                )
            kw_sent_cnt += 1
            total_sent += len(new_arts)
            for a in new_arts:
                sent_db[a.url] = True
        except Exception as e:
            st.error(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨ ({kw}): {e}")

        write_json(SENT_DB_PATH, sent_db)

    return kw_sent_cnt, total_sent

# =========================
# ìŠ¤ì¼€ì¤„ëŸ¬
# =========================
def ensure_scheduler():
    if st.session_state.scheduler is None:
        st.session_state.scheduler = BackgroundScheduler(timezone="Asia/Seoul")
        st.session_state.scheduler.start()

def start_schedule(every_minutes: int):
    ensure_scheduler()
    for job in st.session_state.scheduler.get_jobs():
        st.session_state.scheduler.remove_job(job.id)
    st.session_state.scheduler.add_job(
        func=lambda: _scheduled_send(),
        trigger=IntervalTrigger(minutes=every_minutes),
        id="pe_news_push",
        replace_existing=True,
        max_instances=1,
        coalesce=True
    )
    st.session_state.is_running = True

def stop_schedule():
    if st.session_state.scheduler:
        for job in st.session_state.scheduler.get_jobs():
            st.session_state.scheduler.remove_job(job.id)
    st.session_state.is_running = False

def _scheduled_send():
    try:
        kw_cnt, sent_cnt = do_run_once()
        logging.info("Scheduled run finished: %s keywords, %s articles sent", kw_cnt, sent_cnt)
    except Exception as e:
        logging.exception("Scheduled run error: %s", e)

# =========================
# ë©”ì¸ UI
# =========================
c1, c2, c3 = st.columns(3)
with c1:
    if st.button("ì§€ê¸ˆ í•œ ë²ˆ ì‹¤í–‰"):
        k, t = do_run_once()
        st.session_state.last_run = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        st.success(f"ì™„ë£Œ: {k}ê°œ í‚¤ì›Œë“œ, {t}ê±´ ê¸°ì‚¬ ì²˜ë¦¬{' (ì „ì†¡ ìƒëµ: í…ŒìŠ¤íŠ¸ ëª¨ë“œ)' if st.session_state.get('test_mode', False) else ''}")
with c2:
    if st.button("ìŠ¤ì¼€ì¤„ ì‹œì‘"):
        start_schedule(60)  # ê¸°ë³¸ 60ë¶„ (ì‚¬ì´ë“œë°” interval_minì„ ì‚¬ìš©í•˜ë ¤ë©´ ì—°ê²°)
        st.success("ìŠ¤ì¼€ì¤„ ì‹œì‘")
with c3:
    if st.button("ìŠ¤ì¼€ì¤„ ì¤‘ì§€"):
        stop_schedule()
        st.info("ìŠ¤ì¼€ì¤„ ì¤‘ì§€")

st.divider()
st.subheader("ìƒíƒœ")
st.write(f"Scheduler ì‹¤í–‰ ì¤‘: {st.session_state.is_running}")
if st.session_state.last_run:
    st.write(f"ë§ˆì§€ë§‰ ìˆ˜ë™ ì‹¤í–‰: {st.session_state.last_run}")

# ë¯¸ë¦¬ë³´ê¸°(ì²« í‚¤ì›Œë“œ ìƒìœ„ 10ê±´)
if providers and CONFIG.get("KEYWORDS"):
    try:
        first_kw = CONFIG["KEYWORDS"][0]
        preview: List[Article] = []
        for q in expand_queries(first_kw):
            preview.extend(fetch_articles(providers, q, page_size=int(CONFIG.get("PAGE_SIZE", 30))))

        url_seen = {}
        for a in preview:
            u = normalize_url(a.url)
            if u and u not in url_seen:
                url_seen[u] = a
        preview = list(url_seen.values())
        preview = [a for a in preview if should_keep(
            a, csv_to_list(""), csv_to_list(",".join(CONFIG.get("BLOCK_DOMAINS", []))),
            csv_to_list(",".join(CONFIG.get("INCLUDE_TITLE_KEYWORDS", []))),
            csv_to_list(",".join(CONFIG.get("EXCLUDE_TITLE_KEYWORDS", []))),
            int(CONFIG.get("RECENCY_HOURS", 48))
        )]
        preview = dedup_by_title(preview, score_article)
        preview = sorted(preview, key=lambda x: (score_article(x), x.published_at), reverse=True)[:10]

        st.subheader(f"ë¯¸ë¦¬ë³´ê¸°: â€œ{first_kw}â€ ìƒìœ„ 10ê±´")
        for a in preview:
            st.markdown(
                f"- [{html_unescape(a.title)}]({a.url})  \n  <small>{domain_of(a.url)} â€¢ {a.published_at[:16].replace('T',' ')}</small>",
                unsafe_allow_html=True
            )
    except Exception as e:
        st.warning(f"ë¯¸ë¦¬ë³´ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
else:
    st.info("config.jsonì˜ KEYWORDSê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
