
# -*- coding: utf-8 -*-
"""
pe_monitoring.py â€” êµ­ë‚´ PE ë™í–¥ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ (Telegram ì „ì†¡)

í•µì‹¬ ë³€ê²½ì 
- ì¤‘ë³µ ì œê±° ê°•í™”: "ì •ê·œí™” URL-ID" + "ì •ê·œí™” ì œëª© ì‹œê·¸ë‹ˆì²˜"ë¥¼ ìºì‹œì— í•¨ê»˜ ì €ì¥/ì¡°íšŒ
- ì£¼ê¸°(ì‹œê°„) ê°„ì—ë„ ë™ì¼/ìœ ì‚¬ ê¸°ì‚¬ ì¬ì „ì†¡ ì–µì œ
- ìºì‹œ ë³´ì¡´ê¸°ê°„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°(ê¸°ë³¸ 7ì¼, ì„¤ì • ê°€ëŠ¥)
- ë„¤ì´ë²„/ì¼ë°˜ URL canonicalization ê°•í™” (ëª¨ë°”ì¼/ë°ìŠ¤í¬í†±, ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ì°¨ì´ í—ˆìš©)
- ê°„ë‹¨í•œ ìŠ¤ì¼€ì¤„ëŸ¬ ì˜µì…˜(--schedule-minutes) ë° ë‹¨ë°œ ì‹¤í–‰(--once)

í•„ìš” íŒŒì¼
- config.json: í‚¤ì›Œë“œ/ì œì™¸í‚¤ì›Œë“œ/í…”ë ˆê·¸ë¨ í† í°/ì±„íŒ…ID/ë‰´ìŠ¤ API í‚¤ ë“±

ì‹¤í–‰ ì˜ˆì‹œ
- í•œ ë²ˆë§Œ ì‹¤í–‰:   python pe_monitoring.py --once
- 60ë¶„ ì£¼ê¸°ë¡œ:    python pe_monitoring.py --schedule-minutes 60
"""

import os
import re
import json
import time
import math
import hmac
import hashlib
import logging as log
import argparse
import datetime as dt
from dataclasses import dataclass
from typing import Dict, List, Tuple, Any, Optional
from urllib.parse import urlparse, parse_qs, urlunparse

import requests

# ---------- ì„¤ì • ----------

DEFAULT_CONFIG_PATH = os.environ.get("PE_MONITOR_CONFIG", "config.json")
CACHE_FILE = os.environ.get("PE_MONITOR_CACHE", "sent_cache.json")
CACHE_RETENTION_HOURS = int(os.environ.get("PE_MONITOR_CACHE_RETENTION_HOURS", "168"))  # 7ì¼
MAX_TELEGRAM_ITEMS_PER_MESSAGE = int(os.environ.get("PE_MONITOR_MAX_ITEMS", "30"))
REQUEST_TIMEOUT = (6.0, 12.0)  # (connect timeout, read timeout)

SEOUL_TZ = dt.timezone(dt.timedelta(hours=9))  # Asia/Seoul

# ---------- ìœ í‹¸ ----------

def _utcnow_iso() -> str:
    return dt.datetime.now(dt.timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def _to_seoul_str(when: dt.datetime) -> str:
    if when.tzinfo is None:
        when = when.replace(tzinfo=dt.timezone.utc)
    return when.astimezone(SEOUL_TZ).strftime("%Y-%m-%d %H:%M")

def load_json(path: str, default: Any) -> Any:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        log.warning("JSON ë¡œë“œ ì‹¤íŒ¨(%s): %s", path, e)
        return default

def save_json(path: str, data: Any) -> None:
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning("JSON ì €ì¥ ì‹¤íŒ¨(%s): %s", path, e)

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()

# ---------- ì œëª©/URL ì •ê·œí™” ----------

_RE_NEWS_SUFFIXES = re.compile(r"""
    # í”í•œ ì ‘ë¯¸ì‚¬/íƒœê·¸/ëŒ€ê´„í˜¸ í‘œì‹œ ì œê±°
    [\[\(ã€]?\s?(ë‹¨ë…|ì¢…í•©|ì†ë³´|ë§ˆì¼“ì¸|íŠ¹ì§•ì£¼|ì‹œê·¸ë„|ì•Œì“¸ì‹ ì¡|PEëŠ”\s*ì§€ê¸ˆ)\s?[\]\)ã€‘]?
""", re.IGNORECASE | re.VERBOSE)

_RE_WHITES = re.compile(r"\s+")

def normalize_title(title: str) -> str:
    """
    ì „ì†¡ ì¤‘ë³µ ì–µì œë¥¼ ìœ„í•œ ë³´ìˆ˜ì  ì œëª© ì •ê·œí™”
    - ê´„í˜¸ ë‚´ ìˆ˜ì‹ì–´ ì œê±°([ë‹¨ë…], (ì¢…í•©) ë“±)
    - ê³µë°±/ì—°ì†ê¸°í˜¸ ì •ë¦¬
    - ëŒ€ì†Œë¬¸ì í‘œì¤€í™”
    """
    if not title:
        return ""
    t = title
    # ê´„í˜¸/ë¸Œë¼ì¼“ íƒœê·¸ë¥˜ ì¼ë¶€ ì œê±° (í•„ìš” ì‹œ íŒ¨í„´ í™•ì¥)
    t = _RE_NEWS_SUFFIXES.sub("", t)
    # í•˜ì´í”ˆ/ì½œë¡  ì£¼ë³€ ê³µë°± ì •ë¦¬
    t = t.replace(" - ", " ").replace(" : ", ": ").replace("â€¦", "...")
    # ê³µë°± ì •ê·œí™”
    t = _RE_WHITES.sub(" ", t).strip()
    # ëŒ€ì†Œë¬¸ì í‘œì¤€í™” (í•œê¸€ì—” ì˜í–¥ ê±°ì˜ ì—†ìŒ)
    t = t.lower()
    return t

_RE_NAVER_OID = re.compile(r"[?&]oid=(\d+)")
_RE_NAVER_AID = re.compile(r"[?&]aid=(\d+)")

def canonical_url_id(url: str) -> str:
    """
    ê¸°ì‚¬ URLì„ ì¶œì²˜ ë¶ˆë¬¸í•˜ê³  ì•ˆì •ì ì¸ IDë¡œ ì •ê·œí™”
    - ë„¤ì´ë²„: oid/aid ì¶”ì¶œ â†’ naver:oid:aid
    - ê¸°íƒ€: (scheme ì œê±°) netloc + path (ì¿¼ë¦¬ ì œê±°), ëª¨ë°”ì¼ ì„œë¸Œë„ë©”ì¸ì€ netloc ì •ê·œí™”
    """
    try:
        if not url:
            return ""
        u = url.strip()
        # ë„¤ì´ë²„ ë‰´ìŠ¤: oid / aid ê¸°ë°˜
        if "naver.com" in u and ("/mnews/" in u or "/news/" in u):
            # ëª¨ë°”ì¼/ë°ìŠ¤í¬í†± ë„ë©”ì¸ êµ¬ë¶„ ë¬´ì‹œ
            try:
                qs = parse_qs(urlparse(u).query)
                oid = qs.get("oid", [None])[0]
                aid = qs.get("aid", [None])[0]
                if (not oid or not aid):
                    # ì¼ë¶€ ê²½ë¡œí˜• URLì˜ ê²½ìš° pathì—ì„œ ì¶”ì¶œ
                    # ì˜ˆ: https://n.news.naver.com/mnews/article/018/0006134096
                    p = urlparse(u).path
                    m = re.search(r"/article/(\d+)/(\d+)", p)
                    if m:
                        oid, aid = m.group(1), m.group(2)
                if oid and aid:
                    return f"naver:{oid}:{aid}"
            except Exception:
                pass

        # ê·¸ ì™¸: ìŠ¤í‚´ ì œê±° + ì¿¼ë¦¬ ì œê±° + ëª¨ë°”ì¼ ì„œë¸Œë„ë©”ì¸ ì •ë¦¬
        parts = urlparse(u)
        netloc = parts.netloc.lower()
        # ëª¨ë°”ì¼ í•˜ìœ„ë„ë©”ì¸ ì •ê·œí™” (ì˜ˆ: m.xxx.com â†’ xxx.com)
        if netloc.startswith("m."):
            netloc = netloc[2:]
        path = parts.path
        # ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” /amp, /m ë“± ë³€í˜• ì œê±°
        path = re.sub(r"/amp/?$", "/", path, flags=re.IGNORECASE)
        path = re.sub(r"/m(/|$)", "/", path, flags=re.IGNORECASE)
        return f"{netloc}{path}".rstrip("/")
    except Exception:
        return url

# ---------- ìˆ˜ì§‘ ----------

def fetch_naver_news(keywords: List[str], size_per_kw: int = 15) -> List[Dict[str, Any]]:
    """
    Naver News(ê²€ìƒ‰) í¬ë¡¤ ê¸°ë°˜ ê°„ì´ ìˆ˜ì§‘.
    - ê³µì‹ OpenAPI(ê²€ìƒ‰ë‰´ìŠ¤)ê°€ ìˆë‹¤ë©´ ê·¸ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© ê¶Œì¥.
    - ì—¬ê¸°ì„œëŠ” ì›¹ê²€ìƒ‰ HTMLì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  naver ê¸°ì‚¬ ê³ ì • íŒ¨í„´ì„ ëŒ€ìƒìœ¼ë¡œ ê¸°ì‚¬ ë³¸ë¬¸ URL í˜•íƒœë¥¼ ìš°ì„  ìˆ˜ì§‘.
    ì£¼: ì‹¤ì„œë¹„ìŠ¤ì—ì„  ìì²´ êµ¬í˜„/ì‚¬ë‚´ í”„ë¡ì‹œ ë“± ê¶Œì¥.
    """
    # ì´ ì˜ˆì œëŠ” ê°„ì†Œí™”. ì‹¤ì œë¡œëŠ” News API / ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì‹œê¸¸ ê¶Œì¥.
    items: List[Dict[str, Any]] = []
    for kw in keywords:
        # news APIê°€ ì—†ë‹¤ê³  ê°€ì •í•˜ê³ , ëŒ€ì²´ë¡œ ì‚¬ìš©ìì˜ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ì—ì„œ ìˆ˜ì§‘ëœ ê²°ê³¼ë¥¼ ì´ í•¨ìˆ˜ë¡œ í•©ì¹œë‹¤ê³  ìƒê°í•˜ì„¸ìš”.
        # í•„ìš” ì‹œ ì´ ìë¦¬ë¥¼ ì‚¬ìš©ìì˜ ê¸°ì¡´ ëª¨ë“ˆ í˜¸ì¶œë¡œ êµì²´í•˜ì„¸ìš”.
        # ì˜ˆì‹œì—ì„œëŠ” ë¹ˆ êµ¬í˜„ (ì‹¤ ë°°í¬ í™˜ê²½ì—ì„œ ê¸°ì¡´ ìˆ˜ì§‘ í•¨ìˆ˜ ì´ìš©)
        _ = kw  # placate linter
        pass
    return items

def fetch_newsapi_everything(api_key: str, keywords: List[str], language: str = "ko", page_size: int = 50) -> List[Dict[str, Any]]:
    """
    NewsAPI.org Everything ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (í‚¤ì›Œë“œ OR ìˆ˜ì§‘).
    """
    if not api_key:
        return []
    headers = {"User-Agent": "Mozilla/5.0 (PE-Monitor/1.0)"}
    items: List[Dict[str, Any]] = []
    query = " OR ".join([f"\"{kw}\"" for kw in keywords if kw.strip()])
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": query,
        "language": language,
        "pageSize": page_size,
        "sortBy": "publishedAt",
        "apiKey": api_key,
    }
    try:
        r = requests.get(url, params=params, headers=headers, timeout=REQUEST_TIMEOUT)
        r.raise_for_status()
        data = r.json()
        for a in data.get("articles", []):
            title = a.get("title") or ""
            url_ = a.get("url") or ""
            published_at = a.get("publishedAt")
            if published_at:
                try:
                    when = dt.datetime.fromisoformat(published_at.replace("Z", "+00:00"))
                except Exception:
                    when = dt.datetime.now(dt.timezone.utc)
            else:
                when = dt.datetime.now(dt.timezone.utc)
            items.append({
                "title": title.strip(),
                "url": url_.strip(),
                "published_at": when.astimezone(dt.timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
                "source": (a.get("source") or {}).get("name"),
            })
    except Exception as e:
        log.warning("NewsAPI ìˆ˜ì§‘ ì‹¤íŒ¨: %s", e)
    return items

def collect_all(cfg: Dict[str, Any], env: Dict[str, str]) -> List[Dict[str, Any]]:
    """
    ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í•´ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ê²°í•©.
    - ì‚¬ìš© ì¤‘ì¸ íŒŒì´í”„ë¼ì¸ì— ë§ì¶° ì´ í•¨ìˆ˜ë¥¼ í™•ì¥/êµì²´í•˜ì„¸ìš”.
    """
    keywords = cfg.get("keywords", ["PEF", "ì‚¬ëª¨í€ë“œ", "M&A", "ì¸ìˆ˜í•©ë³‘"])
    use_newsapi = cfg.get("use_newsapi", True)

    items: List[Dict[str, Any]] = []

    if use_newsapi:
        items += fetch_newsapi_everything(env.get("NEWSAPI_KEY", ""), keywords=keywords)

    # ì—¬ê¸°ì— Naver News API/ì‚¬ë‚´ìˆ˜ì§‘ ëª¨ë“ˆ ê²°ê³¼ë¥¼ ë³‘í•©
    # items += fetch_naver_news(keywords)

    # ê°„ë‹¨ ì •ë¦¬: title/url ì—†ëŠ” í•­ëª© ì œê±°
    items = [it for it in items if it.get("title") and it.get("url")]
    return items

# ---------- í•„í„°ë§/ë­í‚¹ ----------

DEFAULT_INCLUDE_KEYWORDS = [
    "PEF", "ì‚¬ëª¨í€ë“œ", "í”„ë¼ì´ë¹—ì—ì¿¼í‹°", "M&A", "ë°”ì´ì•„ì›ƒ", "ë”œ", "ì¸ìˆ˜í•©ë³‘", "ê²½ì˜ê¶Œ ë¶„ìŸ",
    "ì§€ë¶„ ì¸ìˆ˜", "íˆ¬ì ìœ ì¹˜", "í†µë§¤ê°", "ê³µê°œë§¤ê°", "ì˜ˆë¹„ì…ì°°", "ë³¸ì…ì°°", "ë§¤ê°", "ì¶œì",
]

DEFAULT_EXCLUDE_KEYWORDS = [
    "ì—°ì˜ˆ", "ìŠ¤í¬ì¸ ", "ë‚ ì”¨", "ì‚¬ì„¤", "ì¹¼ëŸ¼", "ì˜¤í”¼ë‹ˆì–¸", "ì¦ì‹œ ê¸‰ë“±ë½", "íŠ¹ì§•ì£¼", "ë¦¬ë·°",
]

def _contains_any(text: str, keywords: List[str]) -> bool:
    t = text or ""
    return any(kw.lower() in t.lower() for kw in keywords if kw.strip())

def filter_items(raw_items: List[Dict[str, Any]], cfg: Dict[str, Any]) -> List[Dict[str, Any]]:
    inc = cfg.get("include_keywords", DEFAULT_INCLUDE_KEYWORDS)
    exc = cfg.get("exclude_keywords", DEFAULT_EXCLUDE_KEYWORDS)

    out = []
    for it in raw_items:
        title = it.get("title", "")
        if not _contains_any(title, inc):
            continue
        if _contains_any(title, exc):
            continue
        out.append(it)
    return out

def dedup_within_batch(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ê°™ì€ ë°°ì¹˜ ë‚´ ìœ ì‚¬ì œëª©/ë™ì¼ URL-ID ì œê±° (ë³´ìˆ˜ì )
    """
    seen_ids = set()
    seen_titles = set()
    out = []
    for it in items:
        t_sig = normalize_title(it.get("title", ""))
        cid = canonical_url_id(it.get("url", ""))
        if cid in seen_ids or t_sig in seen_titles:
            continue
        seen_ids.add(cid)
        seen_titles.add(t_sig)
        out.append(it)
    return out

def rank_filtered(items: List[Dict[str, Any]], cfg: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ë‹¨ìˆœ ì •ë ¬: ë°œí–‰ì‹œê° desc â†’ ì œëª© ê¸¸ì´ ì•ˆì •ì„±
    """
    def _k(it):
        ts = it.get("published_at")
        try:
            when = dt.datetime.fromisoformat(ts.replace("Z", "+00:00"))
        except Exception:
            when = dt.datetime.now(dt.timezone.utc)
        return (when, -len(it.get("title", "")))
    items = filter_items(items, cfg)
    items = dedup_within_batch(items)
    return sorted(items, key=_k, reverse=True)

# ---------- ìºì‹œ(ì „ì†¡ ì´ë ¥) ----------

@dataclass
class SentEntry:
    id: str        # canonical_url_id
    title_sig: str # normalize_title
    ts_utc: str    # ISO8601 (UTC)

def load_sent_cache() -> Dict[str, Any]:
    """
    ìºì‹œ í˜•ì‹:
    {
      "entries": [
        {"id": "...", "title_sig": "...", "ts_utc": "....Z"},
        ...
      ],
      "legacy_url_hashes": ["abc...", ...]   # êµ¬ë²„ì „ í˜¸í™˜
    }
    """
    data = load_json(CACHE_FILE, default={"entries": [], "legacy_url_hashes": []})
    # êµ¬ë²„ì „(list)ì¼ ê²½ìš° ë§ˆì´ê·¸ë ˆì´ì…˜
    if isinstance(data, list):
        return {"entries": [], "legacy_url_hashes": data}
    if "entries" not in data:
        data["entries"] = []
    if "legacy_url_hashes" not in data:
        data["legacy_url_hashes"] = []
    return data

def save_sent_cache(cache: Dict[str, Any]) -> None:
    save_json(CACHE_FILE, cache)

def _prune_cache(cache: Dict[str, Any]) -> None:
    cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(hours=CACHE_RETENTION_HOURS)
    kept = []
    for e in cache.get("entries", []):
        try:
            ts = dt.datetime.fromisoformat(e.get("ts_utc", "").replace("Z", "+00:00"))
            if ts.tzinfo is None:
                ts = ts.replace(tzinfo=dt.timezone.utc)
        except Exception:
            ts = cutoff
        if ts >= cutoff:
            kept.append(e)
    # ì•ˆì „ ìƒí•œ
    cache["entries"] = kept[-5000:]

def _is_seen(cache: Dict[str, Any], url: str, title: str) -> bool:
    can_id = canonical_url_id(url)
    t_sig  = normalize_title(title)
    for e in cache.get("entries", []):
        if e.get("id") == can_id or e.get("title_sig") == t_sig:
            return True
    # êµ¬ë²„ì „ í˜¸í™˜ (sha1(url))
    legacy = set(cache.get("legacy_url_hashes", []))
    if sha1(url) in legacy:
        return True
    return False

def _cache_add_all(cache: Dict[str, Any], items: List[Dict[str, Any]]) -> None:
    for it in items:
        cache.setdefault("entries", []).append({
            "id": canonical_url_id(it.get("url","")),
            "title_sig": normalize_title(it.get("title","")),
            "ts_utc": _utcnow_iso(),
        })
    # êµ¬ë²„ì „ í•´ì‹œë„ ìœ ì§€(í›„ë°©í˜¸í™˜)
    legacy = set(cache.get("legacy_url_hashes", []))
    legacy |= {sha1(it.get("url","")) for it in items}
    cache["legacy_url_hashes"] = sorted(list(legacy))[-10000:]
    _prune_cache(cache)
    save_sent_cache(cache)

# ---------- í¬ë§·/í…”ë ˆê·¸ë¨ ----------

def format_telegram_text(items: List[Dict[str, Any]], header: Optional[str] = None) -> str:
    if not header:
        header = "ğŸ“Œ êµ­ë‚´ PE ë™í–¥ ê´€ë ¨ ë‰´ìŠ¤"
    lines = [header]
    for it in items:
        title = it.get("title", "").strip()
        url = it.get("url", "").strip()
        ts = it.get("published_at")
        try:
            when = dt.datetime.fromisoformat(ts.replace("Z", "+00:00"))
        except Exception:
            when = dt.datetime.now(dt.timezone.utc)
        seoul = _to_seoul_str(when)
        lines.append(f"â€¢ {title} ({url}) ({seoul})")
    return "\n".join(lines)

def send_telegram(bot_token: str, chat_id: str, text: str) -> bool:
    if not bot_token or not chat_id:
        log.warning("í…”ë ˆê·¸ë¨ ì„¤ì • ëˆ„ë½(bot_token/chat_id)")
        return False
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": text,
        "disable_web_page_preview": True,
    }
    try:
        r = requests.post(url, json=payload, timeout=REQUEST_TIMEOUT)
        r.raise_for_status()
        return True
    except Exception as e:
        log.warning("í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: %s", e)
        return False

# ---------- ë©”ì¸ ì‹¤í–‰ ----------

def transmit_once(cfg: Dict[str, Any], env: Dict[str, str]) -> Dict[str, Any]:
    raw = collect_all(cfg, env)
    ranked = rank_filtered(raw, cfg)

    cache = load_sent_cache()
    new_items = [it for it in ranked if not _is_seen(cache, it.get("url",""), it.get("title",""))]

    if not new_items:
        send_telegram(env.get("TELEGRAM_BOT_TOKEN",""), env.get("TELEGRAM_CHAT_ID",""), "ğŸ“­ ì‹ ê·œ ë‰´ìŠ¤ ì—†ìŒ")
        return {"count": 0, "items": []}

    # í…”ë ˆê·¸ë¨ ë¶„í•  ì „ì†¡
    sent_any = False
    for i in range(0, len(new_items), MAX_TELEGRAM_ITEMS_PER_MESSAGE):
        chunk = new_items[i:i+MAX_TELEGRAM_ITEMS_PER_MESSAGE]
        text = format_telegram_text(chunk)
        ok = send_telegram(env.get("TELEGRAM_BOT_TOKEN",""), env.get("TELEGRAM_CHAT_ID",""), text)
        sent_any = sent_any or ok
        time.sleep(0.6)

    if sent_any:
        _cache_add_all(cache, new_items)

    return {"count": len(new_items), "items": new_items}

def load_config(path: str = DEFAULT_CONFIG_PATH) -> Dict[str, Any]:
    cfg = load_json(path, default={})
    # ê¸°ë³¸ê°’ ë³´ê°•
    cfg.setdefault("keywords", DEFAULT_INCLUDE_KEYWORDS[:])
    cfg.setdefault("include_keywords", DEFAULT_INCLUDE_KEYWORDS[:])
    cfg.setdefault("exclude_keywords", DEFAULT_EXCLUDE_KEYWORDS[:])
    cfg.setdefault("use_newsapi", True)
    return cfg

def _env() -> Dict[str, str]:
    # í™˜ê²½ë³€ìˆ˜ + config.jsonì˜ telegram, newsapi ì„¤ì • ë³‘í•©
    env = {
        "TELEGRAM_BOT_TOKEN": os.environ.get("TELEGRAM_BOT_TOKEN", ""),
        "TELEGRAM_CHAT_ID": os.environ.get("TELEGRAM_CHAT_ID", ""),
        "NEWSAPI_KEY": os.environ.get("NEWSAPI_KEY", ""),
    }
    # config.jsonì— ë³„ë„ ì§€ì •ì´ ìˆìœ¼ë©´ ë³´ê°•
    cfg = load_config()
    t = cfg.get("telegram", {})
    if t:
        env["TELEGRAM_BOT_TOKEN"] = env["TELEGRAM_BOT_TOKEN"] or t.get("bot_token", "")
        env["TELEGRAM_CHAT_ID"] = env["TELEGRAM_CHAT_ID"] or t.get("chat_id", "")
    n = cfg.get("newsapi", {})
    if n:
        env["NEWSAPI_KEY"] = env["NEWSAPI_KEY"] or n.get("api_key", "")
    return env

def run_scheduler(minutes: int) -> None:
    cfg = load_config()
    env = _env()
    interval = max(5, int(minutes))  # ìµœì†Œ 5ë¶„
    log.info("ìŠ¤ì¼€ì¤„ ì‹œì‘: %dë¶„ ê°„ê²©", interval)
    while True:
        try:
            transmit_once(cfg, env)
        except Exception as e:
            log.exception("ì£¼ê¸° ì‹¤í–‰ ì˜¤ë¥˜: %s", e)
        finally:
            time.sleep(interval * 60)

def main():
    global DEFAULT_CONFIG_PATH
    parser = argparse.ArgumentParser(description="êµ­ë‚´ PE ë™í–¥ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ (Telegram)")
    parser.add_argument("--config", default=DEFAULT_CONFIG_PATH, help="ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: config.json)")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--once", action="store_true", help="í•œ ë²ˆë§Œ ì‹¤í–‰")
    group.add_argument("--schedule-minutes", type=int, help="Në¶„ ê°„ê²©ìœ¼ë¡œ ë¬´í•œ ì‹¤í–‰")
    args = parser.parse_args()

    # ë¡œê¹… ì„¤ì •
    log.basicConfig(
        level=log.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # ì„¤ì • ë¡œë“œ
    global DEFAULT_CONFIG_PATH
    DEFAULT_CONFIG_PATH = args.config
    cfg = load_config(DEFAULT_CONFIG_PATH)
    env = _env()

    if args.once:
        res = transmit_once(cfg, env)
        log.info("ì „ì†¡ ê²°ê³¼: %s", res.get("count"))
    else:
        run_scheduler(args.schedule_minutes)

if __name__ == "__main__":
    main()
