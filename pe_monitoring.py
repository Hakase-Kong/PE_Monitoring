# -*- coding: utf-8 -*-
import json, re, os, time, logging, hashlib, threading, datetime as dt
from typing import List, Dict, Optional
from urllib.parse import urlparse, parse_qs, urlunparse, urlencode, urlsplit, urlunsplit
import requests

# ==============================
# Configuration & Constants
# ==============================

APP_NAME = "PE Monitoring Bot"
CACHE_FILE = "/mnt/data/sent_cache.json"
RUN_LOCK = threading.Lock()

# ë…¸ì´ì¦ˆ íƒœê·¸ (ë¨¸ë¦¬ë§/ê¼¬ë¦¬ë§ ì œê±°ìš©)
NOISE_TAGS = {
    "ë‹¨ë…","ì†ë³´","ì‹œê·¸ë„","fnë§ˆì¼“ì›Œì¹˜","íˆ¬ì360","ì˜ìƒ","í¬í† ","ë¥´í¬","ì‚¬ì„¤","ì¹¼ëŸ¼","ë¶„ì„",
    "ë§ˆì¼“ì¸","PEëŠ” ì§€ê¸ˆ"
}

# ==============================
# Utilities
# ==============================

def now_kst() -> dt.datetime:
    return dt.datetime.now(dt.timezone(dt.timedelta(hours=9)))

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

def load_config(path: str="/mnt/data/config.json") -> Dict:
    if not os.path.exists(path):
        raise FileNotFoundError(f"config.json not found at {path}")
    with open(path, "r", encoding="utf-8") as f:
        cfg = json.load(f)
    return cfg

def get_env_from_cfg(cfg: Dict) -> Dict:
    env = {
        "NAVER_CLIENT_ID": cfg.get("NAVER_CLIENT_ID", ""),
        "NAVER_CLIENT_SECRET": cfg.get("NAVER_CLIENT_SECRET", ""),
        "NEWSAPI_KEY": cfg.get("NEWSAPI_KEY", ""),
        "TELEGRAM_BOT_TOKEN": cfg.get("TELEGRAM_BOT_TOKEN", ""),
        "TELEGRAM_CHAT_ID": cfg.get("TELEGRAM_CHAT_ID", ""),
    }
    return env

def get_run_lock() -> threading.Lock:
    return RUN_LOCK

# ==============================
# Title normalization & URL canonicalization
# ==============================

RE_BRACKET = re.compile(r"^[\[\(ã€ã€ˆ<]\s*([^)\]ã€‘ã€‰>]+)\s*[\)\]ã€‘ã€‰>]\s*")
RE_MULTI_WS = re.compile(r"\s+")

def normalize_title(title: str) -> str:
    """ë¨¸ë¦¬ë§ ê¼¬ë¦¬í‘œ/ë…¸ì´ì¦ˆ ì œê±° + ì†Œë¬¸ìí™” + ê³µë°±ì •ë¦¬"""
    if not title:
        return ""
    t = title.strip()

    # ì•ìª½ ê´„í˜¸/ëŒ€ê´„í˜¸ íƒœê·¸ ë°˜ë³µ ì œê±°
    changed = True
    while changed:
        changed = False
        m = RE_BRACKET.match(t)
        if m:
            tag = m.group(1).strip()
            if tag.replace(" ", "") in {x.replace(" ", "") for x in NOISE_TAGS}:
                t = t[m.end():].lstrip()
                changed = True

    # ì¤‘ê°„ì— í¬í•¨ëœ ë…¸ì´ì¦ˆ ëŒ€ê´„í˜¸ë„ ì œê±°
    for tag in NOISE_TAGS:
        t = re.sub(rf"\s*[\[\(ã€ã€ˆ<]\s*{re.escape(tag)}\s*[\)\]ã€‘ã€‰>]\s*", " ", t, flags=re.IGNORECASE)

    # íŠ¹ìˆ˜ê¸°í˜¸ ê³¼ë‹¤ ì œê±°
    t = re.sub(r"[â€œâ€\"'â€˜â€™Â·â€¢â€¦â–¶â–·â–²â–¼â– â–¡â—†â—‡â€»â˜…â˜†â–â—â“]", " ", t)
    t = RE_MULTI_WS.sub(" ", t).strip().lower()
    return t

def canonical_url_id(url: str) -> Optional[str]:
    """
    ë§¤ì²´ë³„ 'ë‚´ìš© ë™ì¼ì„±'ì„ ìµœëŒ€í•œ ë³´ì¥í•  ìˆ˜ ìˆëŠ” ì •ê·œí™”ëœ id ì¶”ì¶œ
    - Naver News: oid/aid
    - ì¼ë°˜ URL: netloc + path + ì •ë ¬ëœ ì¿¼ë¦¬ key ì¤‘ ì¼ë¶€
    """
    if not url:
        return None
    try:
        u = urlsplit(url)
        host = (u.netloc or "").lower()

        # Naver News ëª¨ë°”ì¼/ë°ìŠ¤í¬í†± ê³µí†µ ì²˜ë¦¬
        if "naver.com" in host and "/mnews" in u.path or "/news" in u.path:
            qs = parse_qs(u.query)
            oid = qs.get("oid", [None])[0]
            aid = qs.get("aid", [None])[0]
            if not (oid and aid):
                # mnews í˜•íƒœ: .../article/{oid}/{aid}
                m = re.search(r"/article/(\d+)/(\d+)", u.path)
                if m:
                    oid, aid = m.group(1), m.group(2)
            if oid and aid:
                return f"naver:{oid}:{aid}"

        # ê¸°íƒ€: host + path (queryëŠ” ì •ë ¬í•œ ì£¼ìš” keyë§Œ í¬í•¨)
        base = f"{host}{u.path}"
        qs = parse_qs(u.query)
        # í”í•œ ì‹ë³„ì í‚¤ë“¤ë§Œ ê³¨ë¼ ì •ë ¬
        keys = ["id","aid","oid","articleId","docid"]
        picked = {k: qs[k] for k in keys if k in qs}
        if picked:
            return f"{base}?{urlencode(sorted([(k, v[0]) for k, v in picked.items()]))}"
        return base
    except Exception:
        return None

def _bigrams(s: str):
    s = s.replace(" ", "")
    return set([s[i:i+2] for i in range(len(s)-1)]) if len(s) >= 2 else set()

def is_near_dup(a: str, b: str) -> bool:
    """ì œëª© ê·¼ì‚¬ì¤‘ë³µ íŒë‹¨: í† í° Jaccardì™€ ë°”ì´ê·¸ë¨ Jaccardë¥¼ í•¨ê»˜ ì‚¬ìš©"""
    if not a or not b:
        return False
    atoks = set(a.split())
    btoks = set(b.split())
    if atoks and btoks:
        jacc = len(atoks & btoks) / max(1, len(atoks | btoks))
        if jacc >= 0.70:
            return True
    ab = _bigrams(a)
    bb = _bigrams(b)
    if ab and bb:
        bj = len(ab & bb) / max(1, len(ab | bb))
        if bj >= 0.55:
            return True
    return False

# ==============================
# Cache (with migration & pruning)
# ==============================

def load_sent_cache() -> List[dict]:
    """
    v1: ["<url_sha1>", ...]
    v2: [{"url_hash":..., "id":..., "t_norm":..., "ts":"%Y-%m-%dT%H:%M:%SZ"}, ...]
    """
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        # v1 -> v2 ë§ˆì´ê·¸ë ˆì´ì…˜
        if isinstance(data, list) and data and isinstance(data[0], str):
            return [{"url_hash": s, "id": None, "t_norm": None, "ts": None} for s in data]
        if isinstance(data, list):
            return data
        return []
    except Exception:
        return []

def save_sent_cache(records: List[dict]) -> None:
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.getLogger().warning("ì „ì†¡ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: %s", e)

def _parse_pub_ts_z(ts: Optional[str]) -> Optional[dt.datetime]:
    if not ts:
        return None
    try:
        # ëª¨ë“  timestampëŠ” UTC Zë¡œ ì €ì¥í•˜ëŠ” ê²ƒì„ ê°€ì •
        return dt.datetime.strptime(ts, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=dt.timezone.utc)
    except Exception:
        return None

def _prune_cache(cache: List[dict], keep_hours: int = 168) -> List[dict]:
    """ìºì‹œë¥¼ ìµœê·¼ keep_hours(ê¸°ë³¸ 7ì¼)ë§Œ ìœ ì§€"""
    if not cache:
        return []
    cutoff = (now_kst() - dt.timedelta(hours=keep_hours)).astimezone(dt.timezone.utc)
    out = []
    for c in cache:
        ts = _parse_pub_ts_z(c.get("ts"))
        if (ts is None) or (ts >= cutoff):  # ts ì—†ìœ¼ë©´ ì¼ë‹¨ ìœ ì§€
            out.append(c)
    # í¬ê¸° ì•ˆì „ì¥ì¹˜(ê³¼ë„ ì„±ì¥ ë°©ì§€)
    if len(out) > 5000:
        out = out[-5000:]
    return out

def is_cached_duplicate(item: dict, cache: List[dict], title_sim_hours: int = 168) -> bool:
    """URL/ì •ê·œí™”URLID/ì •ê·œí™”ì œëª©(ê·¼ì‚¬) ê¸°ì¤€ìœ¼ë¡œ ìºì‹œ ì¤‘ë³µ íŒë‹¨"""
    url = item.get("url", "")
    cid = canonical_url_id(url)
    urlh = sha1(url)
    tnorm = normalize_title(item.get("title", ""))
    its = _parse_pub_ts_z(item.get("publishedAt")) or dt.datetime.now(dt.timezone.utc)

    cutoff = its - dt.timedelta(hours=title_sim_hours)

    for c in cache:
        if urlh and urlh == c.get("url_hash"):
            return True
        if cid and cid == c.get("id"):
            return True
        # ì œëª© ê·¼ì‚¬ì¤‘ë³µì€ ì¼ì • ê¸°ê°„ ë‚´ì—ì„œë§Œ ë¹„êµ(ë„ˆë¬´ ì˜›ë‚  ê¸°ì‚¬ì™€ì˜ ì¶©ëŒ ë°©ì§€)
        cts = _parse_pub_ts_z(c.get("ts"))
        if c.get("t_norm") and cts and cts >= cutoff:
            if is_near_dup(tnorm, c["t_norm"]):
                return True
    return False

# ==============================
# Collectors
# ==============================

def _http_get(url: str, headers: Dict=None, params: Dict=None, timeout: int=10) -> Optional[requests.Response]:
    try:
        resp = requests.get(url, headers=headers, params=params, timeout=timeout)
        if resp.status_code == 200:
            return resp
        return None
    except Exception:
        return None

def _to_zulu(ts: dt.datetime) -> str:
    return ts.astimezone(dt.timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def collect_naver(cfg: Dict) -> List[Dict]:
    """
    Naver Search News API ê¸°ë°˜ ìˆ˜ì§‘
    config:
      KEYWORDS: ["êµ­ë‚´ PE", "ì‚¬ëª¨í€ë“œ", ...]
      MAX_RESULTS_PER_SOURCE: 50
      RECENCY_HOURS: 72
    """
    items = []
    client_id = cfg.get("NAVER_CLIENT_ID", "")
    client_secret = cfg.get("NAVER_CLIENT_SECRET", "")
    if not client_id or not client_secret:
        return items

    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    max_results = int(cfg.get("MAX_RESULTS_PER_SOURCE", 50))
    rec_hours = int(cfg.get("RECENCY_HOURS", 72))

    since = now_kst() - dt.timedelta(hours=rec_hours)
    base = "https://openapi.naver.com/v1/search/news.json"
    for q in cfg.get("KEYWORDS", []):
        start = 1
        fetched = 0
        while fetched < max_results and start <= 1000:
            params = {
                "query": q,
                "display": min(100, max_results - fetched),
                "start": start,
                "sort": "date"
            }
            r = _http_get(base, headers=headers, params=params, timeout=10)
            if not r:
                break
            j = r.json()
            arr = j.get("items", [])
            if not arr:
                break
            for it in arr:
                link = it.get("link") or it.get("originallink") or ""
                title = re.sub("<.*?>", "", it.get("title", ""))  # íƒœê·¸ ì œê±°
                desc = re.sub("<.*?>", "", it.get("description", ""))
                pub = it.get("pubDate")  # ì˜ˆ: Thu, 10 Oct 2025 10:18:00 +0900
                try:
                    pub_dt = dt.datetime.strptime(pub, "%a, %d %b %Y %H:%M:%S %z")
                except Exception:
                    pub_dt = now_kst()
                if pub_dt < since:
                    continue
                items.append({
                    "source": "naver",
                    "query": q,
                    "title": title.strip(),
                    "description": desc.strip(),
                    "url": link,
                    "publishedAt": _to_zulu(pub_dt),
                })
            fetched += len(arr)
            start += len(arr)
            if len(arr) == 0:
                break
            time.sleep(0.1)
    return items

def collect_newsapi(cfg: Dict) -> List[Dict]:
    """
    NewsAPI everything endpoint
    """
    items = []
    key = cfg.get("NEWSAPI_KEY", "")
    if not key:
        return items

    max_results = int(cfg.get("MAX_RESULTS_PER_SOURCE", 50))
    rec_hours = int(cfg.get("RECENCY_HOURS", 72))
    since = (now_kst() - dt.timedelta(hours=rec_hours)).astimezone(dt.timezone.utc)

    base = "https://newsapi.org/v2/everything"
    headers = {"Authorization": key}
    for q in cfg.get("KEYWORDS", []):
        page = 1
        fetched = 0
        while fetched < max_results and page <= 5:
            params = {
                "q": q,
                "language": "ko",
                "pageSize": min(100, max_results - fetched),
                "page": page,
                "sortBy": "publishedAt"
            }
            r = _http_get(base, headers=headers, params=params, timeout=10)
            if not r:
                break
            j = r.json()
            arr = j.get("articles", [])
            if not arr:
                break
            for a in arr:
                title = a.get("title") or ""
                desc = a.get("description") or ""
                url = a.get("url") or ""
                src = a.get("source", {}).get("name") or "unknown"
                pub = a.get("publishedAt")  # UTC ISO
                try:
                    pub_dt = dt.datetime.fromisoformat(pub.replace("Z","+00:00"))
                except Exception:
                    pub_dt = dt.datetime.now(dt.timezone.utc)
                if pub_dt < since.astimezone(dt.timezone.utc):
                    continue
                items.append({
                    "source": f"newsapi:{src}",
                    "query": q,
                    "title": title.strip(),
                    "description": desc.strip(),
                    "url": url,
                    "publishedAt": _to_zulu(pub_dt),
                })
            fetched += len(arr)
            page += 1
            if len(arr) == 0:
                break
            time.sleep(0.1)
    return items

# ==============================
# Filtering, Dedup (intra-run), Ranking
# ==============================

def dedup_intra_run(items: List[Dict]) -> List[Dict]:
    """í•œ ë²ˆì˜ ì‹¤í–‰ ë‚´ ì¤‘ë³µ ì œê±° (URL/ID/ì œëª©ê·¼ì‚¬)"""
    out = []
    seen_urlh = set()
    seen_ids = set()
    seen_titles = []
    for it in sorted(items, key=lambda x: x.get("publishedAt",""), reverse=True):
        url = it.get("url","")
        uid = canonical_url_id(url)
        th = sha1(url) if url else None
        tnorm = normalize_title(it.get("title",""))
        dup = False
        if th and th in seen_urlh: dup = True
        if uid and uid in seen_ids: dup = True
        if not dup:
            # ì œëª© ê·¼ì‚¬ ì¤‘ë³µ í™•ì¸
            for tn in seen_titles[-300:]:  # ìµœê·¼ ê²ƒê³¼ë§Œ ë¹„êµ
                if is_near_dup(tnorm, tn):
                    dup = True
                    break
        if dup:
            continue
        if th: seen_urlh.add(th)
        if uid: seen_ids.add(uid)
        seen_titles.append(tnorm)
        out.append(it)
    return out

def rank_filtered(items: List[Dict], cfg: Dict) -> List[Dict]:
    # ë‹¨ìˆœíˆ ìµœì‹ ìˆœ ì •ë ¬. í•„ìš”ì‹œ ë§¤ì²´ ê°€ì¤‘ì¹˜/í‚¤ì›Œë“œ ì ìˆ˜ ë°˜ì˜ ê°€ëŠ¥
    return sorted(items, key=lambda x: x.get("publishedAt",""), reverse=True)

# ==============================
# Telegram
# ==============================

def format_telegram_text(items: List[Dict]) -> str:
    if not items:
        return "ğŸ“­ ì‹ ê·œ ë‰´ìŠ¤ ì—†ìŒ"
    lines = ["ğŸ“Œ êµ­ë‚´ PE ë™í–¥ ê´€ë ¨ ë‰´ìŠ¤"]
    for it in items[:30]:
        t = it.get("title","").strip()
        u = it.get("url","").strip()
        pub = it.get("publishedAt","")  # Zulu
        # í‘œì‹œìš©: KST ë¡œì»¬ ì‹œê° HH:MM
        try:
            pdt = dt.datetime.strptime(pub, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=dt.timezone.utc).astimezone(dt.timezone(dt.timedelta(hours=9)))
            pstr = pdt.strftime("%Y-%m-%d %H:%M")
        except Exception:
            pstr = ""
        lines.append(f"â€¢ {t} ({u}) ({pstr})")
    return "\n".join(lines)

def send_telegram(bot_token: str, chat_id: str, text: str) -> bool:
    if not bot_token or not chat_id:
        return False
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    data = {
        "chat_id": chat_id,
        "text": text
    }
    try:
        r = requests.post(url, data=data, timeout=10)
        return r.status_code == 200
    except Exception:
        return False

# ==============================
# Collection Orchestrator
# ==============================

def collect_all(cfg: Dict, env: Dict) -> List[Dict]:
    items = []
    # ìˆ˜ì§‘ê¸° ì‹¤í–‰
    items += collect_naver({**cfg, **env})
    items += collect_newsapi({**cfg, **env})
    # ì‹¤í–‰ ë‚´ ì¤‘ë³µ ì œê±°
    items = dedup_intra_run(items)
    return items

def _should_skip_by_time(cfg: Dict) -> bool:
    """ì—…ë¬´ì‹œê°„/ì£¼ë§ ìŠ¤í‚µ ë¡œì§ í•„ìš”ì‹œ í™œì„±í™”"""
    # í•„ìš” ì‹œ cfg["ONLY_BUSINESS_HOURS"]=True & HOURS=[9,18] ë“±ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥
    return False

# ==============================
# Main transmit logic (with cross-run dedup via cache)
# ==============================

def transmit_once(cfg: Dict, env: Dict, preview: bool=False) -> Dict:
    run_lock = get_run_lock()
    if not run_lock.acquire(blocking=False):
        logging.info("ë‹¤ë¥¸ ì‹¤í–‰ì´ ì§„í–‰ ì¤‘ì´ì–´ì„œ ì´ë²ˆ ì£¼ê¸°ëŠ” ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return {"count": 0, "items": []}
    try:
        all_items = collect_all(cfg, env)
        ranked = rank_filtered(all_items, cfg)  # ì£¼ê¸° ë‚´(ë™ì¼ ì‹¤í–‰) ì¤‘ë³µ ì œê±°

        if preview:
            return {"count": len(ranked), "items": ranked}

        if _should_skip_by_time(cfg):
            logging.info("ì‹œê°„ ì •ì±…ì— ì˜í•´ ì „ì†¡ ê±´ë„ˆëœ€ (ì—…ë¬´ì‹œê°„/ì£¼ë§/ê³µíœ´ì¼)")
            return {"count": 0, "items": []}

        cache = _prune_cache(load_sent_cache(), keep_hours=max(72, int(cfg.get("RECENCY_HOURS", 72))*2))

        #è·¨-ì£¼ê¸° ì¤‘ë³µ ì œê±°: ìºì‹œì™€ë„ ê·¼ì‚¬ì¤‘ë³µ ê²€ì‚¬
        new_items = []
        for it in ranked:
            if not is_cached_duplicate(it, cache, title_sim_hours=max(72, int(cfg.get("RECENCY_HOURS", 72))*2)):
                new_items.append(it)

        if not new_items:
            send_telegram(env.get("TELEGRAM_BOT_TOKEN", ""), env.get("TELEGRAM_CHAT_ID", ""), "ğŸ“­ ì‹ ê·œ ë‰´ìŠ¤ ì—†ìŒ")
            return {"count": 0, "items": []}

        BATCH = 30
        sent_any = False
        for i in range(0, len(new_items), BATCH):
            chunk = new_items[i:i+BATCH]
            text = format_telegram_text(chunk)
            ok = send_telegram(env.get("TELEGRAM_BOT_TOKEN", ""), env.get("TELEGRAM_CHAT_ID", ""), text)
            sent_any = sent_any or ok
            time.sleep(0.6)

        if sent_any:
            # ìºì‹œì— URL/ì •ê·œí™”ID/ì •ê·œí™”ì œëª©/ë°œí–‰ì‹œê° ê¸°ë¡
            for it in new_items:
                cache.append({
                    "url_hash": sha1(it.get("url","")),
                    "id": canonical_url_id(it.get("url","")),
                    "t_norm": normalize_title(it.get("title","")),
                    "ts": it.get("publishedAt")
                })
            save_sent_cache(_prune_cache(cache))

        return {"count": len(new_items), "items": new_items}
    finally:
        run_lock.release()

# ==============================
# CLI
# ==============================

if __name__ == "__main__":
    import argparse
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    parser = argparse.ArgumentParser(description=APP_NAME)
    parser.add_argument("--config", default="/mnt/data/config.json", help="config.json ê²½ë¡œ")
    parser.add_argument("--preview", action="store_true", help="ìˆ˜ì§‘/í•„í„° í›„ ë¯¸ë¦¬ë³´ê¸°(ì „ì†¡ ì—†ìŒ)")
    parser.add_argument("--run-once", action="store_true", help="í•œ ë²ˆ ì „ì†¡")
    parser.add_argument("--schedule", type=int, default=0, help="ë¶„ ë‹¨ìœ„ ë°˜ë³µ ì‹¤í–‰(ì˜ˆ: 60)")
    args = parser.parse_args()

    cfg = load_config(args.config)
    env = get_env_from_cfg(cfg)

    if args.preview:
        result = transmit_once(cfg, env, preview=True)
        text = format_telegram_text(result["items"])
        print(text)
    elif args.run-once:
        result = transmit_once(cfg, env, preview=False)
        print(f"sent={result['count']}")
    elif args.schedule and args.schedule > 0:
        interval = args.schedule
        print(f"[{APP_NAME}] ì‹œì‘, ì£¼ê¸°={interval}ë¶„")
        while True:
            try:
                result = transmit_once(cfg, env, preview=False)
                logging.info("cycle done: sent=%d", result["count"])
            except Exception as e:
                logging.exception("cycle error: %s", e)
            time.sleep(interval * 60)
    else:
        # ê¸°ë³¸: preview
        result = transmit_once(cfg, env, preview=True)
        text = format_telegram_text(result["items"])
        print(text)
