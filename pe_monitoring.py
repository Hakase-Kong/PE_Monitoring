import os
import re
import json
import time
import math
import queue
import string
import threading
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse

import requests
import streamlit as st
from apscheduler.schedulers.background import BackgroundScheduler

# =========================
# ì „ì—­ ìƒíƒœ (ëŸ°íƒ€ì„ êµì°¨ ì¤‘ë³µ ì œê±°)
# =========================
RUN_SEEN_URLS = set()
RUN_SEEN_TITLES = set()
last_run_info = {"ts": None, "sent": 0, "picked": 0}

# =========================
# ìœ í‹¸
# =========================
KST = timezone(timedelta(hours=9))

def now_kst():
    return datetime.now(tz=KST)

def normalize_title(t: str) -> str:
    t = t.lower().strip()
    t = re.sub(r"\s+", " ", t)
    t = t.translate(str.maketrans("", "", string.punctuation + "â€œâ€â€˜â€™"))
    return t

def within_working_hours():
    # 08~20 KST
    h = now_kst().hour
    return 8 <= h < 20

def get_config_path():
    candidates = [
        os.environ.get("CONFIG_PATH"),
        "/opt/render/project/src/config.json",  # Render ê¸°ë³¸ ê²½ë¡œ
        os.path.abspath(os.path.join(os.path.dirname(__file__), "config.json"))
    ]
    for p in candidates:
        if p and os.path.exists(p):
            return p
    return None

@st.cache_data(show_spinner=False)
def load_config():
    p = get_config_path()
    if not p:
        return None, {"path": None, "exists": False}
    try:
        with open(p, "r", encoding="utf-8") as f:
            cfg = json.load(f)
        return cfg, {"path": p, "exists": True}
    except Exception as e:
        return None, {"path": p, "exists": False, "err": str(e)}

def env_ok(env_name):
    return bool(os.environ.get(env_name, "").strip())

def to_bool(v, default=False):
    if isinstance(v, bool):
        return v
    if isinstance(v, str):
        return v.lower() in ["1","true","yes","y","on"]
    return default

def get_domain(url):
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""

def title_has_any(title, words):
    t = title.lower()
    return any(w.lower() in t for w in words)

def score_item(item, cfg):
    """ë„ë©”ì¸ ê°€ì¤‘ì¹˜ + ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ + ìµœì‹ ì„± ìŠ¤ì½”ì–´"""
    score = 0.0
    domain = get_domain(item["link"])
    weights = cfg.get("DOMAIN_WEIGHTS", {})
    score += float(weights.get(domain, 1.0))

    # ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ ë¶€ìŠ¤íŠ¸
    if title_has_any(item["title"], cfg.get("FIRM_WATCHLIST", [])):
        score += 1.5

    # ìµœì‹ ì„±(ê°€ê¹Œìš¸ìˆ˜ë¡ ê°€ì )
    if item.get("pub_dt"):
        hrs = (now_kst() - item["pub_dt"]).total_seconds() / 3600.0
        score += max(0.0, 2.0 - (hrs / 24.0))  # 24h ì´ë‚´ë©´ ìµœëŒ€ +2 â†’ ì ì°¨ ê°ì†Œ
    return score

# =========================
# ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
# =========================
def search_naver_news(query, display=30):
    """
    Naver Search API (ë‰´ìŠ¤)
    ENV: NAVER_CLIENT_ID, NAVER_CLIENT_SECRET
    """
    cid = os.environ.get("NAVER_CLIENT_ID", "")
    csc = os.environ.get("NAVER_CLIENT_SECRET", "")
    if not cid or not csc:
        return []

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": cid,
        "X-Naver-Client-Secret": csc,
    }
    params = {
        "query": query,
        "display": min(display, 100),
        "start": 1,
        "sort": "date",
    }
    try:
        r = requests.get(url, headers=headers, params=params, timeout=10)
        r.raise_for_status()
        data = r.json()
        items = []
        for it in data.get("items", []):
            # ì¼ë¶€ ê²°ê³¼ëŠ” originallinkê°€ ì—†ê¸°ë„ í•¨
            link = it.get("link") or it.get("originallink") or ""
            title = re.sub("<.*?>", "", it.get("title", ""))
            desc = re.sub("<.*?>", "", it.get("description", ""))
            # NaverëŠ” pubDate ì˜ˆ: 'Fri, 03 Oct 2025 08:20:00 +0900'
            pub_dt = None
            try:
                pub_dt = datetime.strptime(it.get("pubDate",""), "%a, %d %b %Y %H:%M:%S %z").astimezone(KST)
            except Exception:
                pub_dt = now_kst()
            items.append({
                "title": title,
                "desc": desc,
                "link": link,
                "source": "Naver",
                "pub_dt": pub_dt
            })
        return items
    except Exception:
        return []

def search_newsapi(query, page_size=30):
    """
    NewsAPI (ì„ íƒ)
    ENV: NEWSAPI_KEY
    """
    key = os.environ.get("NEWSAPI_KEY", "")
    if not key:
        return []
    url = "https://newsapi.org/v2/everything"
    from_dt = (now_kst() - timedelta(hours=72)).strftime("%Y-%m-%dT%H:%M:%S")
    params = {
        "q": query,
        "pageSize": min(page_size, 100),
        "from": from_dt,
        "language": "ko",
        "sortBy": "publishedAt",
        "apiKey": key
    }
    try:
        r = requests.get(url, params=params, timeout=12)
        r.raise_for_status()
        data = r.json()
        items = []
        for a in data.get("articles", []):
            pub = a.get("publishedAt")
            try:
                pub_dt = datetime.fromisoformat(pub.replace("Z","+00:00")).astimezone(KST)
            except Exception:
                pub_dt = now_kst()
            items.append({
                "title": a.get("title",""),
                "desc": a.get("description",""),
                "link": a.get("url",""),
                "source": a.get("source",{}).get("name","NewsAPI"),
                "pub_dt": pub_dt
            })
        return items
    except Exception:
        return []

# =========================
# í•„í„°ë§/ì§‘ê³„
# =========================
def filter_and_rank(items, cfg):
    # 1) ì œëª© ì œì™¸ í‚¤ì›Œë“œ
    ex_words = cfg.get("EXCLUDE_TITLE_KEYWORDS", [])
    items = [x for x in items if not title_has_any(x["title"], ex_words)]

    # 2) í¬í•¨ í‚¤ì›Œë“œ(ìˆìœ¼ë©´ +, ì—†ìœ¼ë©´ í†µê³¼ â€” ë„ˆë¬´ ê°•í•˜ê²Œ ì œí•œí•˜ì§€ ì•ŠìŒ)
    inc_words = cfg.get("INCLUDE_TITLE_KEYWORDS", [])
    if inc_words:
        kept = []
        for x in items:
            if title_has_any(x["title"], inc_words):
                kept.append(x)
            else:
                # í¬í•¨ì–´ê°€ ì—†ë”ë¼ë„ ì›Œì¹˜ë¦¬ìŠ¤íŠ¸/ë„ë©”ì¸ ì ìˆ˜ë¡œ ì˜¬ë¼ì˜¬ ìˆ˜ ìˆê²Œ low priorityë¡œ ë‚¨ê¹€
                kept.append(x)
        items = kept

    # 3) ë„ë©”ì¸ í—ˆìš©/ì°¨ë‹¨
    allow = set([d.lower() for d in cfg.get("ALLOW_DOMAINS", [])])
    block = set([d.lower() for d in cfg.get("BLOCK_DOMAINS", [])])
    if allow:
        items = [x for x in items if get_domain(x["link"]) in allow or "naver.com" in get_domain(x["link"])]
    items = [x for x in items if get_domain(x["link"]) not in block]

    # 4) ìµœì‹ ì„±
    recency_h = int(cfg.get("RECENCY_HOURS", 48))
    since_ts = now_kst() - timedelta(hours=recency_h)
    items = [x for x in items if not x.get("pub_dt") or x["pub_dt"] >= since_ts]

    # 5) ì‹¤í–‰ ë‚´ ì¤‘ë³µì œê±° (URL/ì œëª©)
    seen_url = set()
    seen_title = set()
    uniq = []
    for x in items:
        u = x["link"].strip()
        t = normalize_title(x["title"])
        if u in seen_url: 
            continue
        if t in seen_title:
            continue
        seen_url.add(u)
        seen_title.add(t)
        uniq.append(x)
    items = uniq

    # 6) ì‹¤í–‰ ê°„(ê¸€ë¡œë²Œ) ì¤‘ë³µì œê±°
    global RUN_SEEN_URLS, RUN_SEEN_TITLES
    tmp = []
    for x in items:
        u = x["link"].strip()
        t = normalize_title(x["title"])
        if u in RUN_SEEN_URLS or t in RUN_SEEN_TITLES:
            continue
        tmp.append(x)
    items = tmp

    # 7) ìŠ¤ì½”ì–´ë§ ì •ë ¬
    items.sort(key=lambda z: score_item(z, cfg), reverse=True)
    return items

def make_batches(cfg):
    """í‚¤ì›Œë“œë³„ ê²€ìƒ‰ -> í•„í„°/ë­í¬ -> í‚¤ì›Œë“œ ê°„ êµì°¨ì¤‘ë³µ ì œê±°"""
    keywords = cfg.get("KEYWORDS", [])
    aliases = cfg.get("KEYWORD_ALIASES", {})
    page_size = int(cfg.get("PAGE_SIZE", 30))
    max_per_key = int(cfg.get("MAX_PER_KEYWORD", 10))

    picked_by_bucket = {}
    cross_seen_u = set()
    cross_seen_t = set()

    for bucket in keywords:
        q_terms = [bucket]
        q_terms += aliases.get(bucket, [])
        q = " OR ".join(list(dict.fromkeys(q_terms)))  # dup ì œê±°

        raw = []
        # Naver ìš°ì„ 
        raw += search_naver_news(q, display=page_size)
        # ì´í›„ NewsAPI ë³´ê°•(ìˆìœ¼ë©´)
        raw += search_newsapi(q, page_size=page_size)

        filtered = filter_and_rank(raw, cfg)

        # í‚¤ì›Œë“œ ê°„ êµì°¨ ì¤‘ë³µ ì œê±°
        deduped = []
        for it in filtered:
            u = it["link"].strip()
            t = normalize_title(it["title"])
            if u in cross_seen_u or t in cross_seen_t:
                continue
            cross_seen_u.add(u)
            cross_seen_t.add(t)
            deduped.append(it)

        picked_by_bucket[bucket] = deduped[:max_per_key]

    return picked_by_bucket

# =========================
# Telegram
# =========================
def send_telegram_message(text, disable_preview=True):
    token = os.environ.get("TELEGRAM_BOT_TOKEN", "").strip()
    chat_id = os.environ.get("TELEGRAM_CHAT_ID", "").strip()
    if not token or not chat_id:
        return False, "TELEGRAM env ë¯¸ì„¤ì •"
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": bool(disable_preview),
    }
    try:
        r = requests.post(url, data=payload, timeout=10)
        ok = r.status_code == 200 and r.json().get("ok", False)
        if not ok:
            return False, f"TG ì˜¤ë¥˜: {r.text[:200]}"
        return True, "ok"
    except Exception as e:
        return False, str(e)

def format_bucket_message(bucket, items):
    if not items:
        return None
    lines = [f"ğŸ“Œ PE ë™í–¥ ë‰´ìŠ¤ ({bucket})"]
    for it in items[:10]:
        src = it.get("source","")
        when = it.get("pub_dt")
        ts = when.strftime("%Y-%m-%d %H:%M") if when else ""
        lines.append(f"â€¢ {it['title']} ({it['link']}) â€” {src} ({ts})")
    return "\n".join(lines)

def transmit_once(cfg, preview=False):
    global RUN_SEEN_URLS, RUN_SEEN_TITLES, last_run_info
    # ê·¼ë¬´ì‹œê°„ ì œí•œ
    if to_bool(cfg.get("ONLY_WORKING_HOURS", False), False):
        if not within_working_hours():
            return {"picked": 0, "sent": 0, "skipped": "off_hours"}

    batches = make_batches(cfg)
    disable_preview = to_bool(cfg.get("TELEGRAM_DISABLE_PREVIEW", True), True)

    total_picked = sum(len(v) for v in batches.values())
    total_sent = 0

    if not preview:
        # ì‹¤í–‰ ê°„ ì¤‘ë³µ ê¸°ë¡ ì—…ë°ì´íŠ¸
        for arr in batches.values():
            for it in arr:
                RUN_SEEN_URLS.add(it["link"].strip())
                RUN_SEEN_TITLES.add(normalize_title(it["title"]))

        # ì „ì†¡
        for bucket, arr in batches.items():
            msg = format_bucket_message(bucket, arr)
            if not msg:
                continue
            ok, _ = send_telegram_message(msg, disable_preview=disable_preview)
            if ok:
                total_sent += len(arr)

    last_run_info = {"ts": now_kst(), "sent": total_sent, "picked": total_picked}
    return {"picked": total_picked, "sent": total_sent, "skipped": None}

# =========================
# ìŠ¤ì¼€ì¤„ëŸ¬
# =========================
SCHED = BackgroundScheduler(timezone=str(KST))
JOB_ID = "pe_monitoring_job"
JOB_LOCK = threading.Lock()

def start_schedule(cfg):
    with JOB_LOCK:
        if SCHED.get_job(JOB_ID):
            return
        interval = int(cfg.get("TRANSMIT_INTERVAL_MIN", 60))
        SCHED.add_job(lambda: transmit_once(cfg, preview=False),
                      "interval", minutes=interval, id=JOB_ID, max_instances=1)
        if not SCHED.running:
            SCHED.start()

def stop_schedule():
    with JOB_LOCK:
        job = SCHED.get_job(JOB_ID)
        if job:
            job.remove()

def scheduler_running():
    return SCHED.get_job(JOB_ID) is not None

# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="PE ë™í–¥ ë‰´ìŠ¤ â†’ Telegram", page_icon="ğŸ“¨", layout="wide")

cfg, cfg_meta = load_config()

with st.sidebar:
    st.markdown("### ìê²©ì¦ëª… / ì„¤ì •")
    st.caption(f"CONFIG ê²½ë¡œ:\n`{cfg_meta.get('path') or 'ë¯¸ë°œê²¬'}` / ì¡´ì¬: **{cfg_meta.get('exists', False)}**")
    st.divider()
    st.text_input("NewsAPI Key (ì„ íƒ)", value=("â—" * 8 if env_ok("NEWSAPI_KEY") else ""), disabled=True)
    st.text_input("Naver Client ID (ì„ íƒ)", value=("â—" * 8 if env_ok("NAVER_CLIENT_ID") else ""), disabled=True)
    st.text_input("Naver Client Secret (ì„ íƒ)", value=("â—" * 8 if env_ok("NAVER_CLIENT_SECRET") else ""), disabled=True)
    st.text_input("Telegram Bot Token", value=("â—" * 8 if env_ok("TELEGRAM_BOT_TOKEN") else ""), disabled=True)
    st.text_input("Telegram Chat ID", value=os.environ.get("TELEGRAM_CHAT_ID",""), disabled=True)

    st.divider()
    st.markdown("### config.json")
    if cfg:
        st.button("êµ¬ì„± ë¦¬ë¡œë“œ", on_click=lambda: load_config.clear())  # cache ì´ˆê¸°í™”
        st.write("**KEYWORDS (ì½ê¸°ì „ìš©)**")
        st.code(", ".join(cfg.get("KEYWORDS", [])) or "(none)")
        st.number_input("í˜ì´ì§€ë‹¹ ìˆ˜ì§‘ ìˆ˜", min_value=5, max_value=100, step=5,
                        value=int(cfg.get("PAGE_SIZE", 30)), key="ps", disabled=True)
        st.number_input("ì „ì†¡ ê±´ìˆ˜ ì œí•œ(í‚¤ì›Œë“œë³„)", min_value=1, max_value=50, step=1,
                        value=int(cfg.get("MAX_PER_KEYWORD", 10)), key="mx", disabled=True)
        st.number_input("ì „ì†¡ ì£¼ê¸°(ë¶„)", min_value=10, max_value=360, step=10,
                        value=int(cfg.get("TRANSMIT_INTERVAL_MIN", 60)), key="iv", disabled=True)
        st.number_input("ì‹ ì„ ë„(ìµœê·¼ Nì‹œê°„)", min_value=6, max_value=168, step=6,
                        value=int(cfg.get("RECENCY_HOURS", 48)), key="rc", disabled=True)
        st.checkbox("ì—…ë¬´ì‹œê°„(08~20 KST) ë‚´ ì „ì†¡", value=to_bool(cfg.get("ONLY_WORKING_HOURS", True)), disabled=True)
        st.checkbox("ë§í¬ í”„ë¦¬ë·° ë¹„í™œì„±í™”", value=to_bool(cfg.get("TELEGRAM_DISABLE_PREVIEW", True)), disabled=True)
    else:
        st.error("config.jsonì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ/JSON ë¬¸ë²•ì„ í™•ì¸í•˜ì„¸ìš”.")

st.title("ğŸ“¬ PE ë™í–¥ ë‰´ìŠ¤ â†’ Telegram ìë™ ì „ì†¡")
st.caption("Streamlit + NewsAPI/Naver + Telegram + APScheduler")

col1, col2, col3 = st.columns([1,1,1])
with col1:
    if st.button("ì§€ê¸ˆ í•œ ë²ˆ ì‹¤í–‰", use_container_width=True):
        if not cfg:
            st.error("config.jsonì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            res = transmit_once(cfg, preview=True)
            st.success(f"ì™„ë£Œ: {res['picked']}ê±´ ë¯¸ë¦¬ë³´ê¸°, {0}ê±´ ì „ì†¡(ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ)")
with col2:
    if st.button("ìŠ¤ì¼€ì¤„ ì‹œì‘", use_container_width=True):
        if not cfg:
            st.error("config.jsonì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            start_schedule(cfg)
with col3:
    if st.button("ìŠ¤ì¼€ì¤„ ì¤‘ì§€", use_container_width=True):
        stop_schedule()

st.divider()
st.subheader("ìƒíƒœ")

st.write(f"Scheduler ì‹¤í–‰ ì¤‘: **{scheduler_running()}**")
if last_run_info["ts"]:
    st.write(f"ë§ˆì§€ë§‰ ìˆ˜í–‰ ì‹œê°: **{last_run_info['ts'].strftime('%Y-%m-%d %H:%M:%S')}**")
st.info("config.jsonì˜ KEYWORDSê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤." if (cfg and not cfg.get("KEYWORDS")) else "")

# ë¯¸ë¦¬ë³´ê¸° ë¸”ë¡
if cfg:
    st.subheader("ë¯¸ë¦¬ë³´ê¸°: ìµœì‹  10ê±´")
    preview = make_batches(cfg)
    # í‚¤ì›Œë“œ ì„¹ì…˜ë³„ë¡œ ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
    for bucket in cfg.get("KEYWORDS", []):
        items = preview.get(bucket, [])[:10]
        if not items:
            continue
        with st.expander(f"{bucket} â€” {len(items)}ê±´", expanded=False):
            for it in items:
                ts = it["pub_dt"].strftime("%Y-%m-%d %H:%M") if it.get("pub_dt") else ""
                st.markdown(f"- [{it['title']}]({it['link']})  \n  <span style='font-size:12px;color:#888'>{it.get('source','')} â€” {ts}</span>", unsafe_allow_html=True)
