import os
import json
import logging
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timezone, timedelta
from urllib.parse import urlparse

import requests
import streamlit as st

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger

# ===== Logging =====
logging.basicConfig(format="%(asctime)s | %(levelname)s | %(message)s", level=logging.INFO)

# ===== Secrets/Env Helper =====
def get_secret(key: str, default: str = "") -> str:
    try:
        return st.secrets.get(key, default)  # secrets.toml ì—†ìœ¼ë©´ ì˜ˆì™¸ ê°€ëŠ¥
    except Exception:
        return os.environ.get(key, default)

# ===== Small Utils =====
def read_json(path: str, default):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default

def write_json(path: str, data):
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)

def now_utc() -> datetime:
    return datetime.utcnow().replace(tzinfo=timezone.utc)

def normalize_url(u: str) -> str:
    return (u or "").strip().replace("http://", "https://")

def domain_of(url: str) -> str:
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""

def html_unescape(s: str) -> str:
    return (s or "").replace("&quot;", '"').replace("&amp;", "&").replace("&apos;", "'").replace("&lt;", "<").replace("&gt;", ">")

# ===== Persistent Paths =====
STORAGE_DIR = os.environ.get("STORAGE_DIR", ".pe_news_state")
os.makedirs(STORAGE_DIR, exist_ok=True)
SENT_DB_PATH = os.path.join(STORAGE_DIR, "sent_urls.json")

# ===== Config (file-based, KEYWORDS ì „ìš©) =====
CONFIG_PATH = os.environ.get("CONFIG_PATH", "config.json")
DEFAULT_CONFIG = {
    "KEYWORDS": [],
    "ALLOW_DOMAINS": [],
    "BLOCK_DOMAINS": ["sports.naver.com", "m.sports.naver.com"],
    "INCLUDE_TITLE_KEYWORDS": [],
    "EXCLUDE_TITLE_KEYWORDS": [],
    "RECENCY_HOURS": 48,
    "ONLY_WORKING_HOURS": True,
    "TELEGRAM_DISABLE_PREVIEW": True,
    "MAX_PER_KEYWORD": 10,
    "PAGE_SIZE": 30,
}

def load_config(path: str = CONFIG_PATH) -> Dict:
    cfg = DEFAULT_CONFIG.copy()
    file_cfg = read_json(path, default=None)
    if file_cfg is None:
        logging.warning("config.json not found. Using DEFAULT_CONFIG.")
        return cfg
    cfg.update(file_cfg)
    return cfg

CONFIG = load_config()

# ===== Providers =====
@dataclass
class Article:
    title: str
    url: str
    source: str
    published_at: str  # ISO 8601
    description: Optional[str] = None

class NewsProvider:
    def fetch(self, query: str, page_size: int = 20) -> List[Article]:
        raise NotImplementedError

class NewsAPIProvider(NewsProvider):
    def __init__(self, api_key: str):
        self.api_key = api_key
    def fetch(self, query: str, page_size: int = 20) -> List[Article]:
        url = "https://newsapi.org/v2/everything"
        params = {"q": query, "language": "ko", "pageSize": page_size, "sortBy": "publishedAt"}
        r = requests.get(url, params=params, headers={"X-Api-Key": self.api_key}, timeout=20)
        r.raise_for_status()
        data = r.json()
        arts = []
        for a in data.get("articles", []):
            arts.append(Article(
                title=a.get("title") or "",
                url=a.get("url") or "",
                source=(a.get("source") or {}).get("name") or "NewsAPI",
                published_at=(a.get("publishedAt") or now_utc().isoformat()),
                description=a.get("description") or "",
            ))
        return arts

class NaverNewsProvider(NewsProvider):
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id; self.client_secret = client_secret
    def fetch(self, query: str, page_size: int = 20) -> List[Article]:
        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {"X-Naver-Client-Id": self.client_id, "X-Naver-Client-Secret": self.client_secret}
        params = {"query": query, "display": min(page_size, 100), "start": 1, "sort": "date"}
        r = requests.get(url, params=params, headers=headers, timeout=20)
        r.raise_for_status()
        data = r.json()
        from email.utils import parsedate_to_datetime
        arts = []
        for it in data.get("items", []):
            pub = it.get("pubDate") or ""
            try:
                published = parsedate_to_datetime(pub).astimezone(timezone.utc).isoformat()
            except Exception:
                published = now_utc().isoformat()
            arts.append(Article(
                title=it.get("title") or "",
                url=(it.get("link") or it.get("originallink") or ""),
                source="Naver",
                published_at=published,
                description=it.get("description"),
            ))
        return arts

# ===== Telegram =====
def send_telegram_message(bot_token: str, chat_id: str, text: str, disable_web_page_preview=True):
    r = requests.post(
        f"https://api.telegram.org/bot{bot_token}/sendMessage",
        data={"chat_id": chat_id, "text": text, "parse_mode": "HTML", "disable_web_page_preview": disable_web_page_preview},
        timeout=20,
    )
    if r.status_code != 200:
        logging.error("Telegram send failed: %s", r.text)
    r.raise_for_status()
    return r.json()

# ===== Ranking / Filtering =====
QUALITY_WEIGHTS = {
    "thebell.co.kr": 3.0, "investchosun.com": 3.0, "dealsite.co.kr": 2.5, "news.naver.com": 1.0,
    "sports.naver.com": -5.0, "m.sports.naver.com": -5.0,
}

def iso_to_dt(iso_str: str) -> datetime:
    try:
        return datetime.fromisoformat(iso_str.replace("Z", "+00:00"))
    except Exception:
        return now_utc()

def hours_ago(iso_str: str) -> float:
    return (now_utc() - iso_to_dt(iso_str)).total_seconds() / 3600.0

def should_keep(a: Article,
                allow_domains: List[str],
                block_domains: List[str],
                must_include_terms: List[str],
                must_exclude_terms: List[str],
                recency_hours: int) -> bool:
    d = domain_of(a.url)
    if d in block_domains: return False
    if allow_domains and d not in allow_domains: return False
    if recency_hours > 0 and hours_ago(a.published_at) > recency_hours: return False
    title = html_unescape(a.title).lower()
    if must_include_terms and not any(t for t in must_include_terms if t in title): return False
    if any(t for t in must_exclude_terms if t in title): return False
    return True

def score_article(a: Article, domain_weights: Dict[str, float], include_terms: List[str], exclude_terms: List[str]) -> float:
    score = domain_weights.get(domain_of(a.url), 0.0)
    h = max(0.0, min(48.0, hours_ago(a.published_at))); score += (48.0 - h) / 48.0 * 2.0
    title = html_unescape(a.title).lower()
    if any(t for t in include_terms if t in title): score += 1.0
    if any(t for t in exclude_terms if t in title): score -= 2.0
    if len(title) < 10: score -= 1.0
    return score

# ===== Core =====
def compose_message(keyword: str, articles: List[Article]) -> str:
    lines = [f"ğŸ“Œ <b>PE ë™í–¥ ë‰´ìŠ¤ ({keyword})</b>"]
    for a in articles:
        dt = a.published_at[:16].replace("T", " ")
        lines.append(f"â€¢ <a href=\"{a.url}\">{html_unescape(a.title)}</a> â€” <i>{a.source}</i> ({dt})")
    return "\n".join(lines)[:4000]

def fetch_articles(providers: List[NewsProvider], keyword: str, page_size: int) -> List[Article]:
    agg: Dict[str, Article] = {}
    for p in providers:
        try:
            for a in p.fetch(keyword, page_size=page_size):
                u = normalize_url(a.url)
                if u and u not in agg:
                    agg[u] = Article(a.title, u, a.source, a.published_at, a.description)
        except Exception as e:
            logging.warning("Provider fetch error (%s): %s", p.__class__.__name__, e)
    return list(agg.values())

def filter_unsent(articles: List[Article], sent_db: Dict[str, bool], max_items: int) -> List[Article]:
    out = []
    for a in articles:
        if a.url in sent_db: continue
        out.append(a)
        if len(out) >= max_items: break
    return out

# ===== Streamlit State =====
if "scheduler" not in st.session_state: st.session_state.scheduler = None
if "is_running" not in st.session_state: st.session_state.is_running = False
if "last_run" not in st.session_state: st.session_state.last_run = None

st.set_page_config(page_title="PE ë™í–¥ ë‰´ìŠ¤ â†’ Telegram", page_icon="ğŸ“¨", layout="wide")
st.title("ğŸ“¨ PE ë™í–¥ ë‰´ìŠ¤ â†’ Telegram ìë™ ì „ì†¡")
st.caption("í‚¤ì›Œë“œëŠ” ì „ë¶€ config.jsonì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤.")

# ===== Sidebar (í‚¤ì›Œë“œëŠ” ì½ê¸°ì „ìš© í‘œì‹œ) =====
with st.sidebar:
    st.subheader("ìê²©ì¦ëª… / ì„¤ì •")
    default_newsapi      = get_secret("NEWSAPI_KEY")
    default_naver_id     = get_secret("NAVER_CLIENT_ID")
    default_naver_secret = get_secret("NAVER_CLIENT_SECRET")
    default_bot_token    = get_secret("TELEGRAM_BOT_TOKEN")
    default_chat_id      = get_secret("TELEGRAM_CHAT_ID")

    newsapi_key = st.text_input("NewsAPI Key (ì„ íƒ)", value=default_newsapi, type="password")
    naver_client_id = st.text_input("Naver Client ID (ì„ íƒ)", value=default_naver_id, type="password")
    naver_client_secret = st.text_input("Naver Client Secret (ì„ íƒ)", value=default_naver_secret, type="password")

    telegram_bot_token = st.text_input("Telegram Bot Token", value=default_bot_token, type="password")
    telegram_chat_id   = st.text_input("Telegram Chat ID (ë³¸ì¸/ê·¸ë£¹)", value=default_chat_id)

    st.divider()
    st.subheader("config.json")
    st.caption(f"ë¡œë“œ ê²½ë¡œ: {CONFIG_PATH}")
    if st.button("êµ¬ì„± ë¦¬ë¡œë“œ"):
        CONFIG = load_config()
        st.success("config.jsonì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")


    st.text("KEYWORDS (ì½ê¸°ì „ìš©)")
    st.code("\n".join(CONFIG.get("KEYWORDS", [])), language="text")

    # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ëŠ” UIì—ì„œ ë®ì–´ì“°ê¸° ê°€ëŠ¥(ìš´ì˜ í¸ì˜)
    page_size        = st.number_input("í˜ì´ì§€ë‹¹ ìˆ˜ì§‘ ìˆ˜", min_value=5, max_value=100, value=int(CONFIG.get("PAGE_SIZE", 30)), step=5)
    max_per_keyword  = st.number_input("ì „ì†¡ ê±´ìˆ˜ ì œí•œ(í‚¤ì›Œë“œë³„)", min_value=1, max_value=50, value=int(CONFIG.get("MAX_PER_KEYWORD", 10)), step=1)
    interval_min     = st.number_input("ì „ì†¡ ì£¼ê¸°(ë¶„)", min_value=5, max_value=720, value=60, step=5)
    recency_hours    = st.number_input("ì‹ ì„ ë„(ìµœê·¼ Nì‹œê°„)", min_value=0, max_value=168, value=int(CONFIG.get("RECENCY_HOURS", 48)), step=1)
    only_working     = st.checkbox("ì—…ë¬´ì‹œê°„(08â€“20 KST) ë‚´ ì „ì†¡", value=bool(CONFIG.get("ONLY_WORKING_HOURS", True)))
    disable_preview  = st.checkbox("ë§í¬ í”„ë¦¬ë·° ë¹„í™œì„±í™”", value=bool(CONFIG.get("TELEGRAM_DISABLE_PREVIEW", True)))
    test_mode        = st.checkbox("í…ŒìŠ¤íŠ¸ ëª¨ë“œ(ì „ì†¡ ìƒëµ)", value=False)

    def csv(v): return ", ".join([x for x in v if isinstance(x, str)])
    allow_domains_txt = st.text_input("í—ˆìš© ë„ë©”ì¸(ì‰¼í‘œ; ë¹„ìš°ë©´ ì „ì²´ í—ˆìš©)", value=csv(CONFIG.get("ALLOW_DOMAINS", [])))
    block_domains_txt = st.text_input("ì°¨ë‹¨ ë„ë©”ì¸(ì‰¼í‘œ)", value=csv(CONFIG.get("BLOCK_DOMAINS", [])))
    include_terms_txt = st.text_input("í¬í•¨ í‚¤ì›Œë“œ(ì œëª©)", value=csv(CONFIG.get("INCLUDE_TITLE_KEYWORDS", [])))
    exclude_terms_txt = st.text_input("ì œì™¸ í‚¤ì›Œë“œ(ì œëª©)", value=csv(CONFIG.get("EXCLUDE_TITLE_KEYWORDS", [])))

def csv_to_list(s: str) -> List[str]:
    return [x.strip().lower() for x in s.split(",") if x.strip()]

allow_domains = csv_to_list(allow_domains_txt)
block_domains = csv_to_list(block_domains_txt)
include_terms = csv_to_list(include_terms_txt)
exclude_terms = csv_to_list(exclude_terms_txt)

# ===== Providers êµ¬ì„± =====
providers: List[NewsProvider] = []
if newsapi_key: providers.append(NewsAPIProvider(newsapi_key))
if naver_client_id and naver_client_secret: providers.append(NaverNewsProvider(naver_client_id, naver_client_secret))
if not providers: st.warning("ìµœì†Œ í•˜ë‚˜ì˜ ë‰´ìŠ¤ ì œê³µì(NewsAPI ë˜ëŠ” Naver)ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

sent_db: Dict[str, bool] = read_json(SENT_DB_PATH, default={})

# ===== ì‹¤í–‰ ë¡œì§ =====
def do_run_once() -> Tuple[int, int]:
    if not providers:
        st.error("ì„¤ì •ëœ ë‰´ìŠ¤ ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤."); return 0, 0
    if (not telegram_bot_token or not telegram_chat_id) and not test_mode:
        st.error("Telegram í† í°/ì±„íŒ… IDê°€ í•„ìš”í•©ë‹ˆë‹¤."); return 0, 0
    if only_working:
        kst_hour = (datetime.utcnow() + timedelta(hours=9)).hour
        if kst_hour < 8 or kst_hour >= 20:
            logging.info("Skipped by working-hours window (KST %02d)", kst_hour)
            return 0, 0

    total_sent = 0; kw_sent_cnt = 0
    keywords = CONFIG.get("KEYWORDS", [])
    for kw in [k.strip() for k in keywords if k.strip()]:
        raw = fetch_articles(providers, kw, page_size=int(page_size))
        # í•„í„°
        filt = [a for a in raw if should_keep(a, allow_domains, block_domains, include_terms, exclude_terms, int(recency_hours))]
        # ë­í‚¹
        ranked = sorted(filt, key=lambda x: (score_article(x, QUALITY_WEIGHTS, include_terms, exclude_terms), x.published_at), reverse=True)
        # ì‹ ê·œ
        new_arts = filter_unsent(ranked, sent_db, max_items=int(max_per_keyword))
        if not new_arts: continue
        msg = compose_message(kw, new_arts)
        try:
            if test_mode:
                logging.info("[TEST MODE] Would send %d items for '%s'", len(new_arts), kw)
            else:
                send_telegram_message(telegram_bot_token, telegram_chat_id, msg, disable_web_page_preview=disable_preview)
            kw_sent_cnt += 1; total_sent += len(new_arts)
            for a in new_arts: sent_db[a.url] = True
        except Exception as e:
            st.error(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨ ({kw}): {e}")

    write_json(SENT_DB_PATH, sent_db)
    return kw_sent_cnt, total_sent

# ===== ìŠ¤ì¼€ì¤„ëŸ¬ =====
def ensure_scheduler():
    if st.session_state.scheduler is None:
        st.session_state.scheduler = BackgroundScheduler(timezone="Asia/Seoul")
        st.session_state.scheduler.start()

def start_schedule(every_minutes: int):
    ensure_scheduler()
    for job in st.session_state.scheduler.get_jobs():
        st.session_state.scheduler.remove_job(job.id)
    st.session_state.scheduler.add_job(func=lambda: _scheduled_send(),
                                       trigger=IntervalTrigger(minutes=every_minutes),
                                       id="pe_news_push", replace_existing=True,
                                       max_instances=1, coalesce=True)
    st.session_state.is_running = True

def stop_schedule():
    if st.session_state.scheduler:
        for job in st.session_state.scheduler.get_jobs():
            st.session_state.scheduler.remove_job(job.id)
    st.session_state.is_running = False

def _scheduled_send():
    try:
        kw_cnt, sent_cnt = do_run_once()
        logging.info("Scheduled run finished: %s keywords, %s articles sent", kw_cnt, sent_cnt)
    except Exception as e:
        logging.exception("Scheduled run error: %s", e)

# ===== UI =====
c1, c2, c3 = st.columns(3)
with c1:
    if st.button("ì§€ê¸ˆ í•œ ë²ˆ ì‹¤í–‰"):
        k, t = do_run_once()
        st.session_state.last_run = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        st.success(f"ì™„ë£Œ: {k}ê°œ í‚¤ì›Œë“œ, {t}ê±´ ê¸°ì‚¬ ì²˜ë¦¬{' (ì „ì†¡ ìƒëµ: í…ŒìŠ¤íŠ¸ ëª¨ë“œ)' if test_mode else ''}")
with c2:
    if st.button("ìŠ¤ì¼€ì¤„ ì‹œì‘"):
        start_schedule(int(interval_min)); st.success(f"ìŠ¤ì¼€ì¤„ ì‹œì‘: {interval_min}ë¶„ ê°„ê²©")
with c3:
    if st.button("ìŠ¤ì¼€ì¤„ ì¤‘ì§€"):
        stop_schedule(); st.info("ìŠ¤ì¼€ì¤„ ì¤‘ì§€")

st.divider()
st.subheader("ìƒíƒœ")
st.write(f"Scheduler ì‹¤í–‰ ì¤‘: {st.session_state.is_running}")
if st.session_state.last_run: st.write(f"ë§ˆì§€ë§‰ ìˆ˜ë™ ì‹¤í–‰: {st.session_state.last_run}")

# ë¯¸ë¦¬ë³´ê¸°
if providers and CONFIG.get("KEYWORDS"):
    try:
        first_kw = CONFIG["KEYWORDS"][0]
        preview = fetch_articles(providers, first_kw, page_size=int(CONFIG.get("PAGE_SIZE", 30)))
        # ê°„ë‹¨ í•„í„°
        preview = [a for a in preview if should_keep(a, allow_domains, block_domains, include_terms, exclude_terms, int(recency_hours))][:10]
        st.subheader(f"ë¯¸ë¦¬ë³´ê¸°: â€œ{first_kw}â€ ìƒìœ„ 10ê±´")
        for a in preview:
            st.markdown(f"- [{html_unescape(a.title)}]({a.url})  \n  <small>{domain_of(a.url)} â€¢ {a.published_at[:16].replace('T',' ')}</small>", unsafe_allow_html=True)
    except Exception as e:
        st.warning(f"ë¯¸ë¦¬ë³´ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
else:
    st.info("config.jsonì˜ KEYWORDSê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
