import os
import re
import json
import time
import hashlib
import logging
import requests
import datetime as dt
from urllib.parse import urlparse, parse_qs
from typing import Optional, List, Dict, Set

import pytz
import streamlit as st
from apscheduler.schedulers.background import BackgroundScheduler
from threading import Lock

# -------------------------
# ê¸°ë³¸ ì„¤ì • / ì „ì—­ ìƒíƒœ
# -------------------------
APP_TZ = pytz.timezone("Asia/Seoul")
DEFAULT_CONFIG_PATH = os.getenv("PE_CFG", "config.json")
CACHE_FILE = os.getenv("PE_SENT_CACHE", "sent_cache.json")  # ì „ì†¡ ì´ë ¥ ì €ì¥(ì¤‘ë³µ ë°©ì§€)

logging.basicConfig(level=logging.INFO)
log = logging.getLogger("pe_monitor")

# ìŠ¤ì¼€ì¤„ëŸ¬ ì¡ì´ ì°¸ì¡°í•  "í˜„ì¬" êµ¬ì„±/í™˜ê²½ (start_schedule()ì—ì„œ ê°±ì‹ )
CURRENT_CFG_PATH = DEFAULT_CONFIG_PATH
CURRENT_CFG_DICT: Dict = {}     # UIì—ì„œ ì‹œì‘í•  ë•Œ ìŠ¤ëƒ…ìƒ· ì €ì¥ (ì¬ì‹œì‘ ì „ê¹Œì§€ ìœ íš¨)
CURRENT_ENV = {
    "NEWSAPI_KEY": os.getenv("NEWSAPI_KEY", ""),
    "NAVER_CLIENT_ID": os.getenv("NAVER_CLIENT_ID", ""),
    "NAVER_CLIENT_SECRET": os.getenv("NAVER_CLIENT_SECRET", ""),
    "TELEGRAM_BOT_TOKEN": os.getenv("TELEGRAM_BOT_TOKEN", ""),
    "TELEGRAM_CHAT_ID": os.getenv("TELEGRAM_CHAT_ID", ""),
    # NEW: OpenAI
    "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
}

# -------------------------
# ê³µìš© ìœ í‹¸
# -------------------------
def load_config(path: str) -> dict:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        log.warning("config ë¡œë“œ ì‹¤íŒ¨(%s): %s", path, e)
        return {}

def now_kst() -> dt.datetime:
    return dt.datetime.now(APP_TZ)

def clamp(n, lo, hi):
    return max(lo, min(hi, n))

def domain_of(url: str) -> str:
    try:
        return (urlparse(url).netloc or "").replace("www.", "")
    except Exception:
        return ""

def _naver_sid(url: str) -> Optional[str]:
    try:
        q = parse_qs(urlparse(url).query).get("sid", [])
        return q[0] if q else None
    except Exception:
        return None

def sha1(text: str) -> str:
    return hashlib.sha1((text or "").encode("utf-8")).hexdigest()

def is_weekend(kst: dt.datetime) -> bool:
    return kst.weekday() >= 5

def is_holiday(kst: dt.datetime, holidays: List[str]) -> bool:
    ymd = kst.strftime("%Y-%m-%d")
    return ymd in set(holidays or [])

def between_working_hours(kst: dt.datetime, start=8, end=20) -> bool:
    return start <= kst.hour < end

# ===== [Scheduler Helpers] =====
def _map_days_for_cron(days_ui: list[str]) -> str:
    """
    UIì—ì„œ ë°›ì€ ["ë§¤ì¼"] ë˜ëŠ” ["ì›”","ìˆ˜","ê¸ˆ"] ì„ APScheduler 'cron' day_of_week í¬ë§·ìœ¼ë¡œ ë³€í™˜
    """
    if not days_ui or "ë§¤ì¼" in days_ui:
        return "*"  # ë§¤ì¼
    m = {"ì›”":"mon", "í™”":"tue", "ìˆ˜":"wed", "ëª©":"thu", "ê¸ˆ":"fri", "í† ":"sat", "ì¼":"sun"}
    mapped = [m[d] for d in days_ui if d in m]
    return ",".join(mapped) if mapped else "*"

def ensure_cron_job(sched: BackgroundScheduler, cfg: dict):
    job_id = "pe_news_job"
    try:
        sched.remove_job(job_id)
    except Exception:
        pass
    day_of_week = _map_days_for_cron(cfg.get("CRON_DAYS_UI", ["ë§¤ì¼"]))
    hour = int(cfg.get("CRON_HOUR", 9))
    minute = int(cfg.get("CRON_MINUTE", 0))
    sched.add_job(
        scheduled_job, "cron",
        id=job_id, replace_existing=True,
        day_of_week=day_of_week, hour=hour, minute=minute
    )

def ensure_interval_job(sched: BackgroundScheduler, minutes: int):
    job_id = "pe_news_job"
    try:
        sched.remove_job(job_id)
    except Exception:
        pass
    sched.add_job(
        scheduled_job, "interval",
        id=job_id, replace_existing=True,
        minutes=int(minutes),
        next_run_time=now_kst()    # ì‹œì‘ ì¦‰ì‹œ 1íšŒ ì‹¤í–‰
    )

def ensure_scheduled_job(sched: BackgroundScheduler, cfg: dict):
    """
    cfg["SCHEDULE_MODE"]:
      - 'ì£¼ê¸°(ë¶„)': interval ìŠ¤ì¼€ì¤„, ì‹œì‘ ì¦‰ì‹œ 1íšŒ ì „ì†¡
      - 'ìš”ì¼/ì‹œê°(ì£¼ê°„/ë§¤ì¼)': cron ìŠ¤ì¼€ì¤„, ì‹œì‘ ì¦‰ì‹œ ì „ì†¡í•˜ì§€ ì•ŠìŒ
    """
    mode = cfg.get("SCHEDULE_MODE", "ì£¼ê¸°(ë¶„)")
    if mode == "ì£¼ê¸°(ë¶„)":
        ensure_interval_job(sched, int(cfg.get("INTERVAL_MIN", 60)))
    else:
        ensure_cron_job(sched, cfg)

# -------------------------
# ì „ì†¡ ìºì‹œ (ì¤‘ë³µ ë°©ì§€)
# -------------------------
def load_sent_cache() -> Set[str]:
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            arr = json.load(f)
            return set(arr if isinstance(arr, list) else [])
    except Exception:
        return set()

def save_sent_cache(hashes: Set[str]) -> None:
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(sorted(list(hashes)), f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning("ì „ì†¡ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: %s", e)

# -------------------------
# ì™¸ë¶€ API (Naver / NewsAPI)
# -------------------------

# ===== [NEW] Story Key & Enhanced Cache (v2) =====
def story_key(item: dict, cfg: dict | None = None) -> str:
    """
    íšŒì°¨(ì‹œê°„) ê°„ì—ë„ ë™ì¼ ì´ìŠˆë¥¼ 1íšŒë§Œ ì „ì†¡í•˜ê¸° ìœ„í•œ í‚¤.
    - Naver: naver:OID:AID
    - ê¸°íƒ€: normalize_title ê¸°ë°˜
    """
    url = item.get("url", "")
    cid = canonical_url_id(url)
    if cid.startswith("naver:"):
        return cid
    norm_t = normalize_title(item.get("title", ""), cfg)
    return f"title:{sha1(norm_t)}"

def _utcnow_iso() -> str:
    return dt.datetime.now(dt.timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def _parse_iso(s: str) -> dt.datetime:
    try:
        return dt.datetime.strptime(s, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=dt.timezone.utc)
    except Exception:
        return dt.datetime.now(dt.timezone.utc)

def load_sent_cache_v2(retention_hours: int = 72) -> (dict, dict):
    """
    ìºì‹œíŒŒì¼ êµ¬ì¡°(ì‹ ê·œ): {"url": {"<hash>": "iso"}, "story": {"<keyhash>":"iso"}}
    êµ¬ë²„ì „(list ë˜ëŠ” ë‹¨ìˆœ list[str])ë„ í˜¸í™˜.
    ë°˜í™˜: (url_dict, story_dict)
    """
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            raw = json.load(f)
    except Exception:
        raw = {}

    now = dt.datetime.now(dt.timezone.utc)
    limit = now - dt.timedelta(hours=max(6, retention_hours))

    url_map = {}
    story_map = {}

    # êµ¬ë²„ì „(list) í˜¸í™˜: URL í•´ì‹œë§Œ ì¡´ì¬
    if isinstance(raw, list):
        for h in raw:
            url_map[h] = _utcnow_iso()
    elif isinstance(raw, dict):
        url_map = dict(raw.get("url", {}))
        story_map = dict(raw.get("story", {}))

    # ë§Œë£Œ ì •ë¦¬
    def _prune(d: dict) -> dict:
        out = {}
        for k, ts in d.items():
            try:
                ts_dt = _parse_iso(ts)
            except Exception:
                continue
            if ts_dt >= limit:
                out[k] = ts
        return out

    return _prune(url_map), _prune(story_map)

def save_sent_cache_v2(url_map: dict, story_map: dict) -> None:
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump({"url": url_map, "story": story_map}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning("ì „ì†¡ ìºì‹œ ì €ì¥ ì‹¤íŒ¨(v2): %s", e)

def search_naver_news(keyword: str, client_id: str, client_secret: str, recency_hours=72, page_size: int = 30) -> List[dict]:
    if not client_id or not client_secret or not keyword:
        return []
    base = "https://openapi.naver.com/v1/search/news.json"
    params = {"query": keyword, "display": clamp(int(page_size), 10, 100), "sort": "date"}
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    try:
        r = requests.get(base, params=params, headers=headers, timeout=12)
        r.raise_for_status()
        data = r.json()
        items = data.get("items", [])
        res = []
        cutoff = now_kst() - dt.timedelta(hours=recency_hours)
        for it in items:
            link = it.get("link") or it.get("originallink") or ""
            if not link:
                continue
            pubdate = it.get("pubDate")
            try:
                pub_kst = dt.datetime.strptime(pubdate, "%a, %d %b %Y %H:%M:%S %z")
            except Exception:
                pub_kst = now_kst()
            if pub_kst < cutoff:
                continue
            title = re.sub("<.*?>", "", it.get("title") or "")
            res.append({
                "title": title.strip(),
                "url": link.strip(),
                "source": domain_of(link),
                "publishedAt": pub_kst.astimezone(dt.timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
                "origin_keyword": keyword,
                "provider": "naver",
            })
        return res
    except Exception as e:
        log.warning("Naver ì˜¤ë¥˜(%s): %s", keyword, e)
        return []

def search_newsapi(query: str, page_size: int, api_key: str, from_hours: int = 72, cfg: dict = None) -> List[dict]:
    if not api_key or not query:
        return []
    base = "https://newsapi.org/v2/everything"
    from_dt = (dt.datetime.now(dt.timezone.utc) - dt.timedelta(hours=from_hours))
    params = {
        "q": (cfg.get("NEWSAPI_QUERY") if (cfg and cfg.get("NEWSAPI_QUERY")) else query),
        "searchIn": "title",
        "pageSize": clamp(int(cfg.get("PAGE_SIZE", 30)), 10, 100),
        "language": "ko",
        "sortBy": "publishedAt",
        "from": from_dt.strftime("%Y-%m-%dT%H:%M:%SZ"),
        "apiKey": api_key,
    }
    if cfg and cfg.get("NEWSAPI_DOMAINS"):
        params["domains"] = ",".join(cfg["NEWSAPI_DOMAINS"])
    try:
        r = requests.get(base, params=params, timeout=12)
        r.raise_for_status()
        data = r.json()
        arts = data.get("articles", [])
        res = []
        for a in arts:
            title = (a.get("title") or "").strip()
            url = (a.get("url") or "").strip()
            if not title or not url:
                continue
            res.append({
                "title": title,
                "url": url,
                "source": domain_of(url) or (a.get("source", {}) or {}).get("name", ""),
                "publishedAt": (a.get("publishedAt") or "").replace(".000Z", "Z"),
                "origin_keyword": "_newsapi",
                "provider": "newsapi",
            })
        return res
    except Exception as e:
        log.warning("NewsAPI ì˜¤ë¥˜: %s", e)
        return []

# -------------------------
# ì¤‘ë³µ ì œê±° ê°•í™” (URL/ì œëª© ì •ê·œí™” + ê·¼ì‚¬ ì¤‘ë³µ)
# -------------------------
NAVER_ART_RE = re.compile(r"/article/(\d{3})/(\d{10})")
NOISE_TAGS = {"ë‹¨ë…","ì†ë³´","ì‹œê·¸ë„","fnë§ˆì¼“ì›Œì¹˜","íˆ¬ì360","ì˜ìƒ","í¬í† ","ë¥´í¬","ì‚¬ì„¤","ì¹¼ëŸ¼","ë¶„ì„"}
BRACKET_RE   = re.compile(r"[\[\(ï¼ˆ](.*?)[\]\)ï¼‰]")
MULTISPACE_RE = re.compile(r"\s+")
SYNONYM_MAP = {
    "immì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸": "immì¸ë² ",
    "imm ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸": "immì¸ë² ",
    "imm investment": "immì¸ë² ",
    "mergers & acquisitions": "m&a",
    "ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸": "ì¸ë² ",
}


# fallback ì‚¬ì „(í˜¹ì‹œ configì— SYNONYM_MAPì´ ì—†ì„ ë•Œë§Œ ì‚¬ìš©)
_FALLBACK_SYNONYM_MAP = {
    "mergers & acquisitions": "m&a",
    "merger": "m&a",
    "acquisition": "ì¸ìˆ˜",
    "tender offer": "ê³µê°œë§¤ìˆ˜",
    "takeover": "ì¸ìˆ˜",
    "sell-down": "ì§€ë¶„ë§¤ê°",
    "spin-off": "ìŠ¤í•€ì˜¤í”„",
    "carve-out": "ì¹´ë¸Œì•„ì›ƒ",
    "immì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸": "immì¸ë² ",
    "imm investment": "immì¸ë² ",
    "private equity": "pe",
    "ì‚¬ëª¨í€ë“œ": "pe",
}
def canonical_url_id(url: str) -> str:
    """ê°™ì€ ê¸°ì‚¬ë¥¼ ë™ì¼ í‚¤ë¡œ ë¬¶ê¸° ìœ„í•œ ì •ê·œí™” ID ìƒì„±."""
    try:
        u = urlparse(url)
        host = (u.netloc or "").replace("www.", "")
        path = u.path or ""
        if host.endswith("naver.com"):
            m = NAVER_ART_RE.search(path)
            if m:
                oid, aid = m.group(1), m.group(2)
                return f"naver:{oid}:{aid}"
        base = f"{host}{path}".rstrip("/")
        return re.sub(r"/+$", "", base)
    except Exception:
        return url

def normalize_title(t: str, cfg: dict | None = None) -> str:
    if not t:
        return ""
    s = t
    # ê´„í˜¸/ëŒ€ê´„í˜¸ ì•ˆì˜ íƒœê·¸ì„± í† í° ì œê±°
    def _strip_noise(m):
        inner = (m.group(1) or "").strip()
        return "" if any(tag in inner.replace(" ", "") for tag in NOISE_TAGS) else inner
    s = BRACKET_RE.sub(_strip_noise, s)
    # ë¨¸ë¦¬ë§ íƒœê·¸ ì œê±°
    for tag in NOISE_TAGS:
        s = re.sub(rf"^\s*(?:\[{tag}\]|\({tag}\))\s*", "", s, flags=re.IGNORECASE)
    # íŠ¹ìˆ˜ë¬¸ì/ë§ì¤„ì„í‘œ ì •ë¦¬
    s = s.replace("â€¦", " ").replace("ã†", " ").replace("Â·", " ").replace("â€”", " ")
    # ë™ì˜ì–´ í†µì¼ (ì†Œë¬¸ì)
    s_low = s.lower()
# â–¶ configì˜ SYNONYM_MAP ìš°ì„  ì ìš©, ì—†ìœ¼ë©´ fallback
    try:
        synonyms = (cfg or {}).get("SYNONYM_MAP") if isinstance(cfg, dict) else None
    except Exception:
        synonyms = None
    if not isinstance(synonyms, dict) or not synonyms:
        synonyms = _FALLBACK_SYNONYM_MAP
    for k, v in (synonyms or {}).items():
        try:
            s_low = s_low.replace(k, v)
        except Exception:
            pass

    # ìˆ«ì ì½¤ë§ˆ ì •ê·œí™”
    s_low = re.sub(r"\b(\d{1,3}(,\d{3})+|\d+)\b", lambda m: m.group(0).replace(",", ""), s_low)
    # ê³µë°± ì •ë¦¬
    s_low = MULTISPACE_RE.sub(" ", s_low).strip()
    return s_low

def _tokens(s: str) -> set:
    return {w for w in re.split(r"[^0-9a-zA-Zê°€-í£]+", s) if len(w) >= 2}

def _bigrams(s: str) -> set:
    return {s[i:i+2] for i in range(len(s)-1)} if len(s) > 1 else set()

def is_near_dup(a: str, b: str, time_a=None, time_b=None, src_a=None, src_b=None) -> bool:
    """ì •ê·œí™” ì œëª© a,b ê·¼ì ‘ ì¤‘ë³µ íŒë‹¨ (ì¶œì²˜+ì‹œê°„ ë³´ê°•)."""
    if not a or not b:
        return False
    if a == b:
        return True
    ta, tb = _tokens(a), _tokens(b)
    if ta and tb:
        j_tok = len(ta & tb) / max(1, len(ta | tb))
        if j_tok >= 0.70:
            return True
    ba, bb = _bigrams(a), _bigrams(b)
    if ba and bb:
        j_bg = len(ba & bb) / max(1, len(ba | bb))
        if j_bg >= 0.55:
            return True
    if a in b or b in a:
        return True
    # âœ… ì¶”ê°€: ë™ì¼ ì¶œì²˜+12ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ â†’ ë™ì¼ ì´ìŠˆ ê°„ì£¼
    try:
        if src_a and src_b and src_a == src_b and time_a and time_b:
            tdiff = abs((time_a - time_b).total_seconds()) / 3600.0
            if tdiff <= 12 and j_bg >= 0.45:
                return True
    except Exception:
        pass
    return False

def _jaccard_bigrams(s: str) -> float:
    a = _bigrams(s); 
    return 0.0 if not a else len(a)

def _sim_norm_title(a: str, b: str) -> float:
    # í† í°/ë°”ì´ê·¸ë¨ í˜¼í•© ìœ ì‚¬ë„ (0~1)
    ta, tb = _tokens(a), _tokens(b)
    ja = len(ta & tb) / max(1, len(ta | tb)) if ta and tb else 0.0
    ba, bb = _bigrams(a), _bigrams(b)
    jb = len(ba & bb) / max(1, len(ba | bb)) if ba and bb else 0.0
    # í† í° 0.6, ë°”ì´ê·¸ë¨ 0.4 ê°€ì¤‘
    return 0.6 * ja + 0.4 * jb

def dedup(items: List[dict], cfg: dict | None = None) -> List[dict]:
    """
    ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ í›„, ë™ì¼ ì´ìŠˆ(ì œëª© ìœ ì‚¬ + ë™ì¼ ì¶œì²˜ + 12h)ì— ì†í•˜ë©´ ì œê±°.
    """
    def _ts_kst(it):
        try:
            return dt.datetime.strptime(it["publishedAt"], "%Y-%m-%dT%H:%M:%SZ")\
                     .replace(tzinfo=dt.timezone.utc).astimezone(APP_TZ)
        except Exception:
            return now_kst()

    cfg = cfg or {}
    xs_th = float(cfg.get("TITLE_SIM_XSRC", 0.56))
    ss_th = float(cfg.get("TITLE_SIM_SAMESRC", 0.52))
    same_src_hours = int(cfg.get("SAME_SOURCE_WINDOW_HOURS", 12))

    work = sorted(items, key=lambda x: x.get("_score", 0.0), reverse=True)
    out, seen = [], []  # seen: dict(t_norm, src, ts)

    for it in work:
        t_norm = normalize_title(it.get("title", ""), cfg)
        src = domain_of(it.get("url", ""))
        ts = _ts_kst(it)
        is_dup = False

        for s in seen:
            # ë™ì¼ ì¶œì²˜ & 12ì‹œê°„ ì´ë‚´ & ì œëª©ìœ ì‚¬ë„ 0.58â†‘ â†’ ë™ì¼ ì´ìŠˆ ê°„ì£¼
            if s["src"] == src and abs((ts - s["ts"]).total_seconds()) <= same_src_hours*3600:
                if _sim_norm_title(t_norm, s["t_norm"]) >= ss_th:
                    is_dup = True
                    break
            # ì¶œì²˜ ë‹¬ë¼ë„ ì œëª©ì´ ê±°ì˜ ë™ì¼(0.68â†‘)ì´ë©´ ì¤‘ë³µ
            if _sim_norm_title(t_norm, s["t_norm"]) >= 0.68:
                is_dup = True
                break

        if not is_dup:
            out.append(it)
            seen.append({"t_norm": t_norm, "src": src, "ts": ts})

    return out

# -------------------------
# ê·œì¹™ ê¸°ë°˜ í•„í„°/ì •ë ¬
# -------------------------
def should_drop(item: dict, cfg: dict) -> bool:
    url = item.get("url", "")
    title = (item.get("title") or "").strip()
    if not url or not title:
        return True

    src = domain_of(url)
    allow = set(cfg.get("ALLOW_DOMAINS", []) or [])
    block = set(cfg.get("BLOCK_DOMAINS", []) or [])
    allow_strict = bool(cfg.get("ALLOWLIST_STRICT", False))

    if src in block:
        return True
    if allow_strict and allow and (src not in allow):
        return True

    # ë„¤ì´ë²„ ì„¹ì…˜ ì œí•œ
    if "naver.com" in src:
        sids = set(cfg.get("NAVER_ALLOW_SIDS", []) or [])
        if sids:
            sid = _naver_sid(url)
            if sid not in sids:
                return True

    # ì œëª© í¬í•¨/ì œì™¸ í‚¤ì›Œë“œ
    include = (cfg.get("INCLUDE_TITLE_KEYWORDS", []) or [])
    if include and not any(w.lower() in title.lower() for w in include):
        return True
    for w in (cfg.get("EXCLUDE_TITLE_KEYWORDS", []) or []):
        if w and w.lower() in title.lower():
            return True

    # -------------------------------
    # ğŸ”½ ì—¬ê¸°ë¶€í„° ìˆ˜ì •/ì¶”ê°€ ë¶€ë¶„
    # -------------------------------
    # PEF ë§¥ë½ í•„ìˆ˜ ì¡°ê±´ì„ ê¸°ë³¸ ì ìš©í•˜ë˜,
    # 'ì‹ ë¢° ë„ë©”ì¸ + ëª¨í˜¸í•˜ì§€ë§Œ ì¤‘ìš”í•œ í† í°(ë§¤ê°/ê³µê°œë§¤ê°/ì¸ìˆ˜ ë“±)'ì´ë©´ LLMìœ¼ë¡œ ë„˜ê¸°ë„ë¡ ìš°íšŒ í—ˆìš©
    context_any = cfg.get("CONTEXT_REQUIRE_ANY", []) or []
    context = (title + " " + item.get("description", "")).lower()

    has_context = any(k.lower() in context for k in context_any)

    trusted = set(cfg.get("TRUSTED_SOURCES_FOR_FI", cfg.get("ALLOW_DOMAINS", [])) or [])
    amb_tokens = set(t.lower() for t in (cfg.get("STRICT_AMBIGUOUS_TOKENS", []) or []))
    has_ambiguous = any(tok in title.lower() for tok in amb_tokens)

    # ë§¥ë½ ë‹¨ì–´ê°€ ì—†ë‹¤ë©´ â†’ (ì‹ ë¢° ë„ë©”ì¸ AND ëª¨í˜¸í† í°)ì¼ ë•Œë§Œ í†µê³¼ì‹œì¼œ LLMì—ì„œ íŒë‹¨
    if not has_context:
        if not (src in trusted and has_ambiguous):
            return True

    return False

def score_item(item: dict, cfg: dict) -> float:
    # ê°„ë‹¨ ê°€ì¤‘ì¹˜: ë„ë©”ì¸ ê°€ì¤‘ì¹˜ + ì‹ ì„ ë„
    src = domain_of(item.get("url", ""))
    score = float((cfg.get("DOMAIN_WEIGHTS", {}) or {}).get(src, 1.0))
    try:
        ts = item.get("publishedAt")
        pub = dt.datetime.strptime(ts, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=dt.timezone.utc)
    except Exception:
        pub = now_kst()
    hours_ago = (now_kst() - pub.astimezone(APP_TZ)).total_seconds() / 3600.0
    score += max(0.0, 6.0 - (hours_ago / 8.0))
    return score

def rank_filtered(items: List[dict], cfg: dict) -> List[dict]:
    arr = [it for it in items if not should_drop(it, cfg)]
    for it in arr:
        it["_score"] = score_item(it, cfg)
    arr.sort(key=lambda x: x["_score"], reverse=True)
    return dedup(arr, cfg)

# -------------------------
# LLM ê¸°ë°˜ 2ì°¨ í•„í„°
# -------------------------
def _flatten_aliases(cfg: dict) -> List[str]:
    out = []
    for k, v in (cfg.get("KEYWORD_ALIASES", {}) or {}).items():
        out.append(k)
        out.extend(v or [])
    return sorted(set(out))

def _llm_prompt_for_item(item: dict, cfg: dict) -> str:
    kw = cfg.get("KEYWORDS", []) or []
    aliases = _flatten_aliases(cfg)
    firms = cfg.get("FIRM_WATCHLIST", []) or []
    context_any = cfg.get("CONTEXT_REQUIRE_ANY", []) or []
    return f"""
ë‹¹ì‹ ì€ 'êµ­ë‚´ PE ë™í–¥' ê´€ë ¨ ê¸°ì‚¬ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê¸°ì‚¬ê°€ ì‚¬ëª¨í€ë“œ(PEF), GP/LP, ì¬ë¬´ì  íˆ¬ìì(FI)ì˜ íˆ¬ìÂ·ì¸ìˆ˜Â·ë§¤ê°Â·ë¦¬íŒŒì´ë‚¸ì‹± í™œë™ê³¼ ê´€ë ¨ì´ ìˆê±°ë‚˜,
ê·¸ë“¤ì´ ê´€ì—¬í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ ê±°ë˜(M&A, ë§¤ê°, ëŒ€í˜• ìê¸ˆì¡°ë‹¬, ê³µê°œë§¤ìˆ˜)ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.
ë‹¨ìˆœ ì‚°ì—… ë‚´ ì „ëµì  ì¸ìˆ˜ë‚˜ ì¼ë°˜ ê¸°ì—… ì¸ì‚¬Â·ìš´ì˜ ë³´ë„ëŠ” ì œì™¸í•©ë‹ˆë‹¤.

íŒë‹¨ ê¸°ì¤€:
- í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(kw)}
- ë™ì˜ì–´: {', '.join(aliases)}
- ìš´ìš©ì‚¬/FI ì›Œì¹˜ë¦¬ìŠ¤íŠ¸: {', '.join(firms)}
- ë§¥ë½ í‚¤ì›Œë“œ(ìˆìœ¼ë©´ ê°•í•œ ê·¼ê±°): {', '.join(context_any)}

ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í•œ ì¤„ë¡œ ë°˜í™˜:
{{
  "relevant": true|false,
  "confidence": 0.0~1.0,
  "category": "PE deal"|"finance general"|"industry M&A"|"irrelevant",
  "matched": ["ë§¤ì¹­ëœ ë‹¨ì–´ë“¤"],
  "reason": "í•œ ì¤„ ê·¼ê±°"
}}

ê¸°ì‚¬:
- ì œëª©: {item.get('title','')}
- ì¶œì²˜: {domain_of(item.get('url',''))}
- ë§í¬: {item.get('url','')}
"""

def _openai_chat(messages: List[Dict], api_key: str, model: str, max_tokens: int = 300, temperature: float = 0.0) -> str:
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model, "messages": messages, "temperature": temperature, "max_tokens": max_tokens}
    r = requests.post(url, headers=headers, json=payload, timeout=25)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0]["message"]["content"]

def llm_filter_items(items: List[dict], cfg: dict, env: dict) -> List[dict]:
    if not items:
        return items
    if not bool(cfg.get("USE_LLM_FILTER", False)):
        return items

    api_key = env.get("OPENAI_API_KEY", "")
    if not api_key:
        log.warning("LLM í•„í„° í™œì„±í™”ë˜ì–´ ìˆìœ¼ë‚˜ OPENAI_API_KEY ë¯¸ì„¤ì • â†’ ê·œì¹™ê¸°ë°˜ ê²°ê³¼ ì‚¬ìš©")
        return items

    model = cfg.get("LLM_MODEL", "gpt-4o-mini")
    conf_th = float(cfg.get("LLM_CONF_THRESHOLD", 0.7))  # âœ… ì™„í™”
    out = []

    for it in items:
        try:
            user_prompt = _llm_prompt_for_item(it, cfg)
            messages = [
                {"role": "system", "content":
                    "You are a professional news classifier for Private Equity (KR). "
                    "Classify only deals/events where a financial investor (PEF/GP/LP, co-invest, secondary, structured/NAV/pref-equity, PIPE, mezzanine, recap/refi) "
                    "is involved or is plausibly involved. "
                    "If a strategic investor (SI) conducts a pure industry M&A without FI involvement, mark relevant=false (category='industry M&A'). "
                    "Return JSON only."
                },
                {"role": "user", "content": user_prompt},
            ]
            resp = _openai_chat(messages, api_key, model, max_tokens=int(cfg.get("LLM_MAX_TOKENS", 400)))
            j = None
            try:
                j = json.loads(resp.strip())
            except Exception:
                import re as _re
                m = _re.search(r"\{[\s\S]*\}$", resp.strip())
                if m:
                    j = json.loads(m.group(0))

            # âœ… ì™„í™”ëœ ì¡°ê±´: PE deal or finance general ë‘˜ ë‹¤ í—ˆìš©
            cat = (j or {}).get("category", "").lower()
            if (
                isinstance(j, dict)
                and j.get("relevant") is True
                and float(j.get("confidence", 0.0)) >= conf_th
                and cat in {"pe deal", "finance general"}
            ):
                it["_llm"] = j
                out.append(it)

        except Exception as e:
            log.warning("LLM í•„í„° ì²˜ë¦¬ ì‹¤íŒ¨: %s", e)
            out.append(it)

    return out

# -------------------------
# LLM ê¸°ë°˜ 2ì°¨ ì¤‘ë³µ ì œê±° (ë™ì¼ ê¸°ì‚¬/ì´ìŠˆ íŒì •)
# -------------------------
def _llm_is_same_story(a: dict, b: dict, env: dict, cfg: dict) -> Optional[bool]:
    """
    ë‘ ê¸°ì‚¬ê°€ 'ê°™ì€ ìŠ¤í† ë¦¬(ë™ì¼ ê¸°ì‚¬/ë™ì¼ ì´ìŠˆ ì¬ë³´ë„)'ì¸ì§€ LLMìœ¼ë¡œ íŒì •.
    True: ë™ì¼ ìŠ¤í† ë¦¬, False: ë‹¤ë¥¸ ìŠ¤í† ë¦¬, None: íŒì • ì‹¤íŒ¨(ë³´ë¥˜)
    """
    api_key = env.get("OPENAI_API_KEY", "")
    if not api_key:
        return None

    def _fmt(it: dict) -> str:
        t = (it.get("title") or "").strip()
        u = (it.get("url") or "").strip()
        s = (domain_of(u) or it.get("source") or "").strip()
        when = it.get("publishedAt") or ""
        return f"- ì œëª©: {t}\n- ì¶œì²˜: {s}\n- ì‹œê°(UTC): {when}\n- ë§í¬: {u}"

    sys = (
        "You are a professional news deduplication judge for Korean finance news.\n"
        "Task: Decide if two news items are about the SAME story (same underlying article/issue), "
        "even if titles differ slightly (follow-ups, minor edits, copy on portal vs. source). "
        "Consider: title meaning, named entities, seller/buyer, price/size, and timing.\n"
        "Answer strictly in JSON: {\"same\": true|false, \"reason\": \"...\"}"
    )
    usr = "ê¸°ì‚¬ A\n" + _fmt(a) + "\n\nê¸°ì‚¬ B\n" + _fmt(b) + "\n\níŒì •:"

    try:
        resp = _openai_chat(
            [{"role": "system", "content": sys}, {"role": "user", "content": usr}],
            api_key,
            cfg.get("LLM_MODEL", "gpt-4o-mini"),
            max_tokens=200,
            temperature=0.0,
        )
        j = None
        try:
            j = json.loads(resp.strip())
        except Exception:
            m = re.search(r"\{[\s\S]*\}$", resp.strip())
            if m:
                j = json.loads(m.group(0))
        if isinstance(j, dict) and "same" in j:
            return bool(j.get("same"))
    except Exception as e:
        log.warning("LLM dedup ì˜¤ë¥˜: %s", e)
    return None


def _utc_to_kst(ts: str) -> dt.datetime:
    try:
        return dt.datetime.strptime(ts, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=dt.timezone.utc).astimezone(APP_TZ)
    except Exception:
        return now_kst()


def llm_dedup_items(items: List[dict], cfg: dict, env: dict) -> List[dict]:
    """
    1ì°¨ ê·œì¹™ê¸°ë°˜ dedup ì´í›„ì—ë„ ë‚¨ëŠ” 'ìœ ì‚¬í•˜ì§€ë§Œ ì• ë§¤í•œ' ì¼€ì´ìŠ¤ë¥¼ LLMìœ¼ë¡œ ë‹¤ì‹œ í•œ ë²ˆ ì •ë¦¬.
    - í›„ë³´ í˜ì–´: ì œëª©ìœ ì‚¬ë„ 0.50~0.72 êµ¬ê°„ ë˜ëŠ” ë™ì¼ì¶œì²˜Â±ì‹œê°„ì°½ ë‚´ ê¸°ì‚¬
    - ë™ì¼ ìŠ¤í† ë¦¬ë¡œ íŒì •ë˜ë©´ _score ë‚®ì€ ìª½ì„ ì œê±°
    """
    if not items or not cfg.get("USE_LLM_DEDUP", False):
        return items
    if not env.get("OPENAI_API_KEY"):
        return items

    win_hours = int(cfg.get("LLM_DEDUP_WINDOW_HOURS", 24))
    keep_mask = [True] * len(items)

    # ì‚¬ì „ ê³„ì‚°: ì •ê·œí™” ì œëª©/ì¶œì²˜/ì‹œê°
    meta = []
    for it in items:
        tnorm = normalize_title(it.get("title", ""), cfg)
        src = domain_of(it.get("url", ""))
        ts_kst = _utc_to_kst(it.get("publishedAt", ""))
        sc = float(it.get("_score", 0.0))
        meta.append((tnorm, src, ts_kst, sc))

    n = len(items)
    for i in range(n):
        if not keep_mask[i]:
            continue
        for j in range(i + 1, n):
            if not keep_mask[j]:
                continue

            a_t, a_s, a_ts, a_sc = meta[i]
            b_t, b_s, b_ts, b_sc = meta[j]

            # ì‹œê°„ì°½ & ì¶œì²˜ ì¡°ê±´
            time_ok = abs((a_ts - b_ts).total_seconds()) <= win_hours * 3600
            src_same = (a_s == b_s)

            # ì œëª© ìœ ì‚¬ë„
            sim = _sim_norm_title(a_t, b_t)

            # LLM íŒì • í›„ë³´ ì¡°ê±´(ë„ˆë¬´ ëª…í™•/ë„ˆë¬´ ë‹¤ë¥¸ ê²ƒì€ ì œì™¸)
            if (0.50 <= sim < 0.72) or (src_same and time_ok and sim >= 0.45):
                same = _llm_is_same_story(items[i], items[j], env, cfg)
                if same is True:
                    # ë‚®ì€ ì ìˆ˜ ìª½ì„ ì œê±°
                    if a_sc >= b_sc:
                        keep_mask[j] = False
                    else:
                        keep_mask[i] = False
                        break  # iê°€ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ ë‚´ë¶€ ë£¨í”„ íƒˆì¶œ
                elif same is False:
                    continue
                else:
                    # íŒì • ì‹¤íŒ¨ëŠ” ë³´ë¥˜
                    continue

    return [it for k, it in enumerate(items) if keep_mask[k]]

# -------------------------
# ìˆ˜ì§‘/ì „ì†¡
# -------------------------
def collect_all(cfg: dict, env: dict) -> List[dict]:
    keywords = cfg.get("KEYWORDS", []) or []
    page_size = int(cfg.get("PAGE_SIZE", 30))
    recency_hours = int(cfg.get("RECENCY_HOURS", 72))

    all_items: List[dict] = []

    # Naver
    for kw in keywords:
        batch = search_naver_news(
            kw, env.get("NAVER_CLIENT_ID",""), env.get("NAVER_CLIENT_SECRET",""),
            recency_hours=recency_hours, page_size=int(cfg.get("PAGE_SIZE", 30))
        )
        all_items += batch
    
    # NewsAPI (ì„ íƒ)
    if env.get("NEWSAPI_KEY") and keywords:
        query = " OR ".join(keywords)
        batch = search_newsapi(query, page_size=page_size, api_key=env["NEWSAPI_KEY"], from_hours=recency_hours, cfg=cfg)
        all_items += batch

    return all_items

def format_telegram_text(items: List[dict], cfg: dict = {} ) -> str:
    if not items:
        return "ğŸ“­ ì‹ ê·œ ë‰´ìŠ¤ ì—†ìŒ"
    lines = ["ğŸ“Œ <b>êµ­ë‚´ PE ë™í–¥ ê´€ë ¨ ë‰´ìŠ¤</b>"]
    for it in items:
        t = it.get("title", "").strip()
        u = it.get("url", "")
        src = domain_of(u)
        try:
            pub = dt.datetime.strptime(it["publishedAt"], "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=dt.timezone.utc).astimezone(APP_TZ)
            when = pub.strftime("%Y-%m-%d %H:%M")
        except Exception:
            when = "-"
        if bool(cfg.get("SHOW_SOURCE_DOMAIN", False)):
            lines.append(f"â€¢ <a href=\"{u}\">{t}</a> â€” {src} ({when})")
        else:
            lines.append(f"â€¢ <a href=\"{u}\">{t}</a> ({when})")
    return "\n".join(lines)

def send_telegram(bot_token: str, chat_id: str, text: str, disable_preview: bool = True) -> bool:
    if not bot_token or not chat_id:
        return False
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    payload = {"chat_id": chat_id, "text": text, "parse_mode": "HTML",
               "disable_web_page_preview": disable_preview}
    try:
        r = requests.post(url, json=payload, timeout=12)
        r.raise_for_status()
        return True
    except Exception as e:
        log.warning("í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: %s", e)
        return False

def _should_skip_by_time(cfg: dict) -> bool:
    kst_now = now_kst()
    if cfg.get("ONLY_WORKING_HOURS") and not between_working_hours(kst_now, 6, 20):  # 08â†’06
        return True
    if cfg.get("BLOCK_WEEKEND") and is_weekend(kst_now):
        return True
    if cfg.get("BLOCK_HOLIDAY") and is_holiday(kst_now, cfg.get("HOLIDAYS", [])):
        return True
    return False

# -------------------------
# ì‹¤í–‰ ê²¹ì¹¨ ë°©ì§€ ë½
# -------------------------
@st.cache_resource(show_spinner=False)
def get_run_lock() -> Lock:
    return Lock()

def transmit_once(cfg: dict, env: dict, preview=False) -> dict:
    run_lock = get_run_lock()
    if not run_lock.acquire(blocking=False):
        log.info("ë‹¤ë¥¸ ì‹¤í–‰ì´ ì§„í–‰ ì¤‘ì´ì–´ì„œ ì´ë²ˆ ì£¼ê¸°ëŠ” ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return {"count": 0, "items": []}
    try:
        all_items = collect_all(cfg, env)
        ranked = rank_filtered(all_items, cfg)  # 1ì°¨: ê·œì¹™ ê¸°ë°˜ í•„í„°
        ranked = pe_focus_filter(ranked, cfg) # PE ì„ íƒ í•„í„°
        ranked = llm_filter_items(ranked, cfg, env)  # 2ì°¨: LLM í•„í„° (ì˜µì…˜)
        ranked = llm_dedup_items(ranked, cfg, env)   # 3ì°¨: LLM ì¤‘ë³µíŒì • (ì˜µì…˜) â† âœ… ì—¬ê¸° ì¶”ê°€

        if preview:
            return {"count": len(ranked), "items": ranked}

        if _should_skip_by_time(cfg):
            log.info("ì‹œê°„ ì •ì±…ì— ì˜í•´ ì „ì†¡ ê±´ë„ˆëœ€ (ì—…ë¬´ì‹œê°„/ì£¼ë§/ê³µíœ´ì¼)")
            return {"count": 0, "items": []}

        retention = int(cfg.get("CACHE_RETENTION_HOURS", cfg.get("RECENCY_HOURS", 72)))
        url_cache, story_cache = load_sent_cache_v2(retention_hours=retention)
        now_iso = _utcnow_iso()

        new_items = []
        for it in ranked:
            uhash = sha1(it.get("url", ""))
            skey  = story_key(it, cfg)
            if (uhash in url_cache) or (skey in story_cache):
                continue
            new_items.append(it)

        if not new_items:
            if bool(cfg.get("NO_NEWS_SILENT", True)):
                log.info("ì‹ ê·œ ë‰´ìŠ¤ ì—†ìŒ (ë¬´ì†Œì‹ ì•Œë¦¼ ì–µì œ ì˜µì…˜ìœ¼ë¡œ ë¯¸ì „ì†¡)")
                return {"count": 0, "items": []}
            else:
                send_telegram(env.get("TELEGRAM_BOT_TOKEN", ""), env.get("TELEGRAM_CHAT_ID", ""), "ğŸ“­ ì‹ ê·œ ë‰´ìŠ¤ ì—†ìŒ")
                return {"count": 0, "items": []}

        BATCH = 30
        sent_any = False
        for i in range(0, len(new_items), BATCH):
            chunk = new_items[i:i+BATCH]
            text = format_telegram_text(chunk, cfg)
            ok = send_telegram(env.get("TELEGRAM_BOT_TOKEN", ""), env.get("TELEGRAM_CHAT_ID", ""), text, disable_preview=bool(cfg.get("TELEGRAM_DISABLE_PREVIEW", True)))
            sent_any = sent_any or ok
            time.sleep(0.6)

        if sent_any:
            for it in new_items:
                url_cache[sha1(it.get("url", ""))] = now_iso
                story_cache[story_key(it, cfg)] = now_iso
            save_sent_cache_v2(url_cache, story_cache)

        return {"count": len(new_items), "items": new_items}
    finally:
        run_lock.release()

# -------------------------
# ìŠ¤ì¼€ì¤„ëŸ¬ (rerun-safe)
# -------------------------
@st.cache_resource(show_spinner=False)
def get_scheduler() -> BackgroundScheduler:
    sched = BackgroundScheduler(timezone=APP_TZ)
    sched.start()
    return sched

def scheduled_job():
    # UIì—ì„œ ìŠ¤ëƒ…ìƒ·ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©
    cfg = CURRENT_CFG_DICT or load_config(CURRENT_CFG_PATH)
    try:
        transmit_once(cfg, CURRENT_ENV, preview=False)
    except Exception as e:
        log.exception("ìŠ¤ì¼€ì¤„ ì‘ì—… ì‹¤íŒ¨: %s", e)

def is_running(_: BackgroundScheduler = None) -> bool:
    try:
        sched = get_scheduler()
        return any(j.id == "pe_news_job" for j in sched.get_jobs())
    except Exception:
        return False

# ìŠ¤ì¼€ì¤„ ì‹œì‘/ì¤‘ì§€(ë²„íŠ¼ í•¸ë“¤ëŸ¬ìš©) â€” ì—¬ê¸°ì„œë§Œ global ì‚¬ìš©
def start_schedule(cfg_path: str, cfg_dict: dict, env: dict, minutes: int):
    """
    ë²„íŠ¼ í´ë¦­ ì‹œ ìŠ¤ì¼€ì¤„ ì‹œì‘.
    - ì£¼ê¸°(ë¶„) ëª¨ë“œ: ìŠ¤ì¼€ì¤„ ë“±ë¡ + ì¦‰ì‹œ 1íšŒ ì „ì†¡
    - ìš”ì¼/ì‹œê° ëª¨ë“œ: ìŠ¤ì¼€ì¤„ ë“±ë¡ë§Œ (ì¦‰ì‹œ ì „ì†¡í•˜ì§€ ì•ŠìŒ)
    """
    global CURRENT_CFG_PATH, CURRENT_CFG_DICT, CURRENT_ENV
    CURRENT_CFG_PATH = cfg_path
    CURRENT_CFG_DICT = dict(cfg_dict)
    CURRENT_ENV = env

    sched = get_scheduler()
    ensure_scheduled_job(sched, CURRENT_CFG_DICT)

def stop_schedule():
    sched = get_scheduler()
    try:
        sched.remove_job("pe_news_job")
    except Exception:
        pass

# ===== [Filter] íŠ¹ì • PE í¬ì»¤ìŠ¤ =====
def pe_focus_filter(items: list[dict], cfg: dict) -> list[dict]:
    """
    cfg['PE_FOCUS']ê°€ ë¹„ì–´ ìˆì§€ ì•Šìœ¼ë©´, ì œëª©/ìš”ì•½/ë³¸ë¬¸ì—
    í•´ë‹¹ í‚¤ì›Œë“œ(PEëª…)ê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ëœ ê¸°ì‚¬ë§Œ í†µê³¼ì‹œí‚µë‹ˆë‹¤.
    items: ê° ì›ì†ŒëŠ” {"title","summary","content","url",...}
    """
    focus = [s.strip() for s in cfg.get("PE_FOCUS", []) if isinstance(s, str) and s.strip()]
    if not focus:
        return items

    def _hit(text: str) -> bool:
        if not text:
            return False
        low = text.lower()
        for kw in focus:
            if kw.lower() in low:
                return True
        return False

    out = []
    for it in items:
        text = f"{it.get('title','')} {it.get('summary','')} {it.get('content','')}"
        if _hit(text):
            out.append(it)
    return out

# -------------------------
# Streamlit UI
# -------------------------
st.set_page_config(page_title="PE ë™í–¥ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§", page_icon="ğŸ“°", layout="wide")

# Config / ìê²©ì¦ëª…
cfg_path = st.sidebar.text_input("config.json ê²½ë¡œ", value=DEFAULT_CONFIG_PATH)
cfg_file = load_config(cfg_path)
st.sidebar.caption(f"Config ë¡œë“œ ìƒíƒœ: {'âœ…' if cfg_file else 'âŒ'}  Â· ê²½ë¡œ: {cfg_path}")

# íŒŒì¼ì˜ ê¸°ë³¸ê°’ì„ UI ëŸ°íƒ€ì„ cfgë¡œ ë³µì‚¬
cfg = dict(cfg_file)

naver_id = st.sidebar.text_input("Naver Client ID", type="password", value=os.getenv("NAVER_CLIENT_ID", ""))
naver_secret = st.sidebar.text_input("Naver Client Secret", type="password", value=os.getenv("NAVER_CLIENT_SECRET", ""))
newsapi_key = st.sidebar.text_input("NewsAPI Key (ì„ íƒ)", type="password", value=os.getenv("NEWSAPI_KEY", ""))
# NEW: OpenAI
openai_key = st.sidebar.text_input("OpenAI API Key (ì„ íƒ)", type="password", value=os.getenv("OPENAI_API_KEY", ""))
bot_token = st.sidebar.text_input("Telegram Bot Token", type="password", value=os.getenv("TELEGRAM_BOT_TOKEN", ""))
chat_id = st.sidebar.text_input("Telegram Chat ID (ì±„ë„/ê·¸ë£¹)", value=os.getenv("TELEGRAM_CHAT_ID", ""))

# íŒŒë¼ë¯¸í„°
st.sidebar.divider()
st.sidebar.subheader("ì „ì†¡/ìˆ˜ì§‘ íŒŒë¼ë¯¸í„°")
cfg["PAGE_SIZE"] = int(st.sidebar.number_input("í˜ì´ì§€ë‹¹ ìˆ˜ì§‘ ìˆ˜", min_value=10, max_value=100, step=1, value=int(cfg.get("PAGE_SIZE", 30))))
cfg["RECENCY_HOURS"] = int(st.sidebar.number_input("ì‹ ì„ ë„(ìµœê·¼ Nì‹œê°„)", min_value=6, max_value=168, step=6, value=int(cfg.get("RECENCY_HOURS", 72))))

# âœ… ì‹œê°„ ì •ì±… í† ê¸€
st.sidebar.subheader("ì‹œê°„ ì •ì±…")
cfg["ONLY_WORKING_HOURS"] = bool(st.sidebar.checkbox("âœ… ì—…ë¬´ì‹œê°„(06~20 KST) ë‚´ ì „ì†¡", value=bool(cfg.get("ONLY_WORKING_HOURS", True))))
cfg["BLOCK_WEEKEND"]     = bool(st.sidebar.checkbox("ğŸš« ì£¼ë§ ë¯¸ì „ì†¡", value=bool(cfg.get("BLOCK_WEEKEND", False))))
cfg["BLOCK_HOLIDAY"]     = bool(st.sidebar.checkbox("ğŸš« ê³µíœ´ì¼ ë¯¸ì „ì†¡", value=bool(cfg.get("BLOCK_HOLIDAY", False))))
holidays_text = st.sidebar.text_area("ê³µíœ´ì¼(YYYY-MM-DD, ì‰¼í‘œ ë˜ëŠ” ì¤„ë°”ê¿ˆ êµ¬ë¶„)", value=", ".join(cfg.get("HOLIDAYS", [])))
cfg["HOLIDAYS"] = [s.strip() for s in re.split(r"[,\n]", holidays_text) if s.strip()]

st.sidebar.subheader("ì „ì†¡ ìŠ¤ì¼€ì¤„")

cfg["SCHEDULE_MODE"] = st.sidebar.radio("ìŠ¤ì¼€ì¤„ ë°©ì‹", options=["ì£¼ê¸°(ë¶„)", "ìš”ì¼/ì‹œê°(ì£¼ê°„/ë§¤ì¼)"], index=0 if cfg.get("SCHEDULE_MODE", "ì£¼ê¸°(ë¶„)") == "ì£¼ê¸°(ë¶„)" else 1, horizontal=False)
if cfg["SCHEDULE_MODE"] == "ì£¼ê¸°(ë¶„)":
    cfg["INTERVAL_MIN"] = int(st.sidebar.number_input(
        "ì „ì†¡ ì£¼ê¸°(ë¶„)",
        min_value=5, max_value=10080, step=5,
        value=int(cfg.get("INTERVAL_MIN", 60))
    ))
    st.sidebar.caption("ìµœëŒ€ 1ì£¼ì¼(=10080ë¶„)ê¹Œì§€ ì„¤ì • ê°€ëŠ¥. ìŠ¤ì¼€ì¤„ ì‹œì‘ ì‹œ ì¦‰ì‹œ 1íšŒ ì „ì†¡ í›„ ì£¼ê¸°ì ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.")

# (ì‹ ê·œ) ìš”ì¼/ì‹œê° ìŠ¤ì¼€ì¤„: ì£¼ 1íšŒ ë˜ëŠ” ë§¤ì¼
else:
    # ì„ íƒ ê°€ëŠ¥í•œ ìš”ì¼
    WEEKDAY_LABELS = ["ë§¤ì¼", "ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
    # ê¸°ë³¸ ì„ íƒê°’
    default_days = cfg.get("CRON_DAYS_UI", ["ë§¤ì¼"])
    # UI: ë‹¤ì¤‘ ì„ íƒ
    selected_days = st.sidebar.multiselect(
        "ì „ì†¡ ìš”ì¼ ì„ íƒ (ë§¤ì¼ì„ ì„ íƒí•˜ë©´ ë‹¤ë¥¸ ìš”ì¼ì€ ë¬´ì‹œë©ë‹ˆë‹¤)",
        options=WEEKDAY_LABELS,
        default=default_days
    )
    # ê²°ê³¼ ì €ì¥ (UI í‘œê¸° ê·¸ëŒ€ë¡œë„ ë³´ì¡´)
    cfg["CRON_DAYS_UI"] = selected_days[:] if selected_days else ["ë§¤ì¼"]

    # ì‹œê°„ ì„ íƒ
    t = st.sidebar.time_input(
        "ì „ì†¡ ì‹œê° (KST)",
        value=dt.time(hour=int(cfg.get("CRON_HOUR", 9)), minute=int(cfg.get("CRON_MINUTE", 0)))
    )
    cfg["CRON_HOUR"] = int(t.hour)
    cfg["CRON_MINUTE"] = int(t.minute)

    st.sidebar.caption("ìš”ì¼/ì‹œê° ëª¨ë“œì—ì„œëŠ” 'ìŠ¤ì¼€ì¤„ ì‹œì‘'ì„ ëˆŒëŸ¬ë„ ì¦‰ì‹œ ì „ì†¡í•˜ì§€ ì•Šê³  ì§€ì • ì‹œê°ì—ë§Œ ì „ì†¡í•©ë‹ˆë‹¤.")

# ê¸°íƒ€ í•„í„° í† ê¸€
st.sidebar.subheader("ê¸°íƒ€ í•„í„°")
cfg["ALLOWLIST_STRICT"] = bool(st.sidebar.checkbox("ğŸ§± ALLOWLIST_STRICT (í—ˆìš© ë„ë©”ì¸ ì™¸ ì°¨ë‹¨)", value=bool(cfg.get("ALLOWLIST_STRICT", False))))

# NEW: LLM í•„í„° ì˜µì…˜
st.sidebar.subheader("LLM í•„í„°(ì„ íƒ)")
cfg["USE_LLM_FILTER"] = bool(st.sidebar.checkbox("ğŸ¤– OpenAIë¡œ 2ì°¨ í•„í„°ë§", value=bool(cfg.get("USE_LLM_FILTER", False))))
cfg["LLM_MODEL"] = st.sidebar.text_input("ëª¨ë¸", value=cfg.get("LLM_MODEL", "gpt-4o-mini"))
cfg["LLM_CONF_THRESHOLD"] = float(st.sidebar.slider("ì±„íƒ ì„ê³„ì¹˜(ì‹ ë¢°ë„)", min_value=0.0, max_value=1.0, value=float(cfg.get("LLM_CONF_THRESHOLD", 0.7)), step=0.05))
cfg["LLM_MAX_TOKENS"] = int(st.sidebar.number_input("max_tokens", min_value=64, max_value=1000, step=10, value=int(cfg.get("LLM_MAX_TOKENS", 300))))

# ===== [UI] íŠ¹ì • PE ì„ íƒ =====
st.sidebar.subheader("ğŸ¯ íŠ¹ì • PE ì„ íƒ(ì„ íƒ)")
# í›„ë³´ ëª©ë¡ì€ config.jsonì˜ "PE_CANDIDATES"ë¥¼ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ì•„ë˜ ê¸°ë³¸
pe_candidates_default = [
    "MBK", "IMM", "Hahn&Co", "VIG", "ê¸€ëœìš°ë“œ", "ë² ì¸", "KKR", "Carlyle", "í•œì•¤ì½”",
    "ìŠ¤í‹±", "H&Q", "ë¸Œë¦¿ì§€", "JIP", "Affinity", "TPG", "KCGI", "í•œí™”", "ë§¥ì¿¼ë¦¬"
]
pe_candidates = cfg.get("PE_CANDIDATES", pe_candidates_default)
cfg["PE_FOCUS"] = st.sidebar.multiselect(
    "íŠ¹ì • PEë§Œ ì„ ë³„(ë¹„ì›Œë‘ë©´ ì „ì²´)",
    options=pe_candidates,
    default=cfg.get("PE_FOCUS", [])
)

st.sidebar.divider()
if st.sidebar.button("êµ¬ì„± ë¦¬ë¡œë“œ", use_container_width=True):
    st.rerun()

st.title("ğŸ“° êµ­ë‚´ PE ë™í–¥ ë‰´ìŠ¤ ìë™ ëª¨ë‹ˆí„°ë§")
st.caption("Streamlit + Naver/NewsAPI + OpenAI Filter + Telegram + APScheduler (Render + UptimeRobot)")

def make_env() -> dict:
    return {
        "NAVER_CLIENT_ID": naver_id,
        "NAVER_CLIENT_SECRET": naver_secret,
        "NEWSAPI_KEY": newsapi_key,
        "OPENAI_API_KEY": openai_key,
        "TELEGRAM_BOT_TOKEN": bot_token,
        "TELEGRAM_CHAT_ID": chat_id,
    }

col1, col2, col3, col4 = st.columns(4)
sched = get_scheduler()

with col1:
    if st.button("ì§€ê¸ˆ í•œë²ˆ ì‹¤í–‰(ë¯¸ë¦¬ë³´ê¸°)", type="primary"):
        res = transmit_once(cfg, make_env(), preview=True)
        st.session_state["preview"] = res

with col2:
    if st.button("ì§€ê¸ˆ í•œë²ˆ ì „ì†¡"):
        res = transmit_once(cfg, make_env(), preview=False)
        st.session_state["preview"] = res

with col3:
    if st.button("ìŠ¤ì¼€ì¤„ ì‹œì‘"):
        start_schedule(cfg_path=cfg_path, cfg_dict=cfg, env=make_env(), minutes=int(cfg["INTERVAL_MIN"]))
        if cfg.get("SCHEDULE_MODE","ì£¼ê¸°(ë¶„)") == "ì£¼ê¸°(ë¶„)":
            st.success("ìŠ¤ì¼€ì¤„ ì‹œì‘ë¨: ì¦‰ì‹œ 1íšŒ ì „ì†¡ í›„ ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰")
        else:
            st.success("ìŠ¤ì¼€ì¤„ ì‹œì‘ë¨: ì§€ì •ëœ ìš”ì¼/ì‹œê°ì—ë§Œ ì „ì†¡(ì‹œì‘ ì¦‰ì‹œ ì „ì†¡ ì—†ìŒ)")
        st.rerun()

with col4:
    if st.button("ìŠ¤ì¼€ì¤„ ì¤‘ì§€"):
        stop_schedule()
        st.warning("ìŠ¤ì¼€ì¤„ ì¤‘ì§€ë¨")
        st.rerun()

# ìƒíƒœ
_running = is_running()
st.subheader("ìƒíƒœ")
sched = get_scheduler()
jobs = []
try:
    jobs = sched.get_jobs()
except Exception:
    jobs = []
st.info(f"Scheduler ì‹¤í–‰ ì¤‘: {_running}")
for j in jobs:
    st.caption(f"â€¢ Job: {j.id} / ë‹¤ìŒ ì‹¤í–‰: {j.next_run_time}")

# ë¯¸ë¦¬ë³´ê¸° ê²°ê³¼ â€” ì „ì²´ í•„í„°ë§ ê¸°ì‚¬ë§Œ í‘œì‹œ (Top10 ì—†ìŒ)
st.subheader("ğŸ“‹ í•„í„°ë§ëœ ì „ì²´ ê¸°ì‚¬")
res = st.session_state.get("preview", {"items": []})
items = res.get("items", [])
if not items:
    st.write("ê²°ê³¼ ì—†ìŒ")
else:
    for it in items:
        t = it.get("title", "")
        u = it.get("url", "")
        try:
            pub = dt.datetime.strptime(it["publishedAt"], "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=dt.timezone.utc).astimezone(APP_TZ)
            when = pub.strftime("%Y-%m-%d %H:%M")
        except Exception:
            when = "-"
        meta = it.get("_llm")
        if meta:
            _m = ", ".join([str(x) for x in (meta.get("matched") or [])][:6])
            st.markdown(
                f"- <a href='{u}'>{t}</a> ({when})  "
                f"<span style='color:gray'>LLM: {meta.get('confidence',0):.2f}"
                + (f" Â· {_m}" if _m else "")
                + "</span>",
                unsafe_allow_html=True
            )
        else:
            st.markdown(f"- <a href='{u}'>{t}</a> ({when})", unsafe_allow_html=True)
