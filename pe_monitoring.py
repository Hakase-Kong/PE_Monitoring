import os
import re
import json
import string
import threading
from datetime import datetime, timedelta
from urllib.parse import urlparse

import requests
import streamlit as st
from apscheduler.schedulers.background import BackgroundScheduler
from zoneinfo import ZoneInfo

# =========================
# ì „ì—­ ìƒíƒœ
# =========================
RUN_SEEN_URLS = set()
RUN_SEEN_TITLES = set()
last_run_info = {"ts": None, "sent": 0, "picked": 0, "note": ""}

# =========================
# ìœ í‹¸
# =========================
KST = ZoneInfo("Asia/Seoul")

def now_kst():
    return datetime.now(tz=KST)

def normalize_title(t: str) -> str:
    t = t.lower().strip()
    t = re.sub(r"\s+", " ", t)
    # ê¸°ë³¸ êµ¬ë‘ì  ì œê±°
    return t.translate(str.maketrans("", "", string.punctuation + "â€œâ€â€˜â€™"))

def get_domain(url):
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""

def title_has_any(title, words):
    tl = title.lower()
    return any(w.lower() in tl for w in words)

def to_bool(v, default=False):
    if isinstance(v, bool):
        return v
    if isinstance(v, str):
        return v.lower() in ["1", "true", "yes", "y", "on"]
    return default

def get_config_path():
    candidates = [
        os.environ.get("CONFIG_PATH"),
        "/opt/render/project/src/config.json",  # Render ê¸°ë³¸ ê²½ë¡œ
        os.path.abspath(os.path.join(os.path.dirname(__file__), "config.json")),
    ]
    for p in candidates:
        if p and os.path.exists(p):
            return p
    return None

@st.cache_data(show_spinner=False)
def load_config():
    p = get_config_path()
    if not p:
        return None, {"path": None, "exists": False}
    try:
        with open(p, "r", encoding="utf-8") as f:
            cfg = json.load(f)
        return cfg, {"path": p, "exists": True}
    except Exception as e:
        return None, {"path": p, "exists": False, "err": str(e)}

def env_ok(env_name):
    return bool(os.environ.get(env_name, "").strip())

# ê·¼ë¬´ì‹œê°„/íœ´ì¼ ì •ì±…
def is_weekend(dt):
    return dt.weekday() >= 5  # 5=í† , 6=ì¼

def is_holiday(dt, cfg):
    days = set(cfg.get("HOLIDAYS_KR", []))
    return dt.strftime("%Y-%m-%d") in days

def within_send_window(cfg):
    if not to_bool(cfg.get("ONLY_WORKING_HOURS", True), True):
        return True
    now = now_kst()
    hour_ok = 8 <= now.hour < 20
    if not hour_ok:
        return False
    if to_bool(cfg.get("SKIP_WEEKENDS", True), True) and is_weekend(now):
        return False
    if to_bool(cfg.get("SKIP_HOLIDAYS", True), True) and is_holiday(now, cfg):
        return False
    return True

# ì ìˆ˜
def score_item(item, cfg):
    score = 0.0
    # ë„ë©”ì¸ ê°€ì¤‘ì¹˜
    weights = cfg.get("DOMAIN_WEIGHTS", {})
    score += float(weights.get(get_domain(item["link"]), 1.0))
    # ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ ë¶€ìŠ¤íŠ¸
    if title_has_any(item["title"], cfg.get("FIRM_WATCHLIST", [])):
        score += 1.5
    # ìµœì‹ ì„± ê°€ì (24h ì´ë‚´ ìµœëŒ€ +2)
    if item.get("pub_dt"):
        hrs = (now_kst() - item["pub_dt"]).total_seconds() / 3600.0
        score += max(0.0, 2.0 - (hrs / 24.0))
    return score

# =========================
# ìˆ˜ì§‘ê¸°
# =========================
def search_naver_news(query, display=30):
    cid = os.environ.get("NAVER_CLIENT_ID", "")
    csc = os.environ.get("NAVER_CLIENT_SECRET", "")
    if not cid or not csc:
        return []
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csc}
    params = {"query": query, "display": min(display, 100), "start": 1, "sort": "date"}
    try:
        r = requests.get(url, headers=headers, params=params, timeout=10)
        r.raise_for_status()
        data = r.json()
        out = []
        for it in data.get("items", []):
            link = it.get("link") or it.get("originallink") or ""
            title = re.sub("<.*?>", "", it.get("title", ""))
            desc = re.sub("<.*?>", "", it.get("description", ""))
            try:
                pub_dt = datetime.strptime(it.get("pubDate", ""), "%a, %d %b %Y %H:%M:%S %z").astimezone(KST)
            except Exception:
                pub_dt = now_kst()
            out.append({"title": title, "desc": desc, "link": link, "source": "Naver", "pub_dt": pub_dt})
        return out
    except Exception:
        return []

def search_newsapi(query, page_size=30):
    key = os.environ.get("NEWSAPI_KEY", "")
    if not key:
        return []
    url = "https://newsapi.org/v2/everything"
    from_dt = (now_kst() - timedelta(hours=72)).strftime("%Y-%m-%dT%H:%M:%S")
    params = {"q": query, "pageSize": min(page_size, 100), "from": from_dt,
              "language": "ko", "sortBy": "publishedAt", "apiKey": key}
    try:
        r = requests.get(url, params=params, timeout=12)
        r.raise_for_status()
        data = r.json()
        out = []
        for a in data.get("articles", []):
            try:
                pub_dt = datetime.fromisoformat(a.get("publishedAt","").replace("Z","+00:00")).astimezone(KST)
            except Exception:
                pub_dt = now_kst()
            out.append({
                "title": a.get("title",""),
                "desc": a.get("description",""),
                "link": a.get("url",""),
                "source": a.get("source",{}).get("name","NewsAPI"),
                "pub_dt": pub_dt
            })
        return out
    except Exception:
        return []

# =========================
# í•„í„°ë§/ì§‘ê³„
# =========================
def filter_and_rank(items, cfg):
    # 1) ì œëª© ì œì™¸
    ex = cfg.get("EXCLUDE_TITLE_KEYWORDS", [])
    items = [x for x in items if not title_has_any(x["title"], ex)]
    # 2) ë„ë©”ì¸ ì°¨ë‹¨/í—ˆìš©
    allow = set([d.lower() for d in cfg.get("ALLOW_DOMAINS", [])])
    block = set([d.lower() for d in cfg.get("BLOCK_DOMAINS", [])])
    strict = to_bool(cfg.get("ALLOWLIST_STRICT", False), False)
    if strict and allow:
        items = [x for x in items if get_domain(x["link"]) in allow]
    items = [x for x in items if get_domain(x["link"]) not in block]
    # 3) ì‹ ì„ ë„
    recency_h = int(cfg.get("RECENCY_HOURS", 48))
    since = now_kst() - timedelta(hours=recency_h)
    items = [x for x in items if not x.get("pub_dt") or x["pub_dt"] >= since]
    # 4) ì‹¤í–‰ ë‚´ ì¤‘ë³µ ì œê±°(ì œëª©/URL)
    seen_u, seen_t, uniq = set(), set(), []
    for x in items:
        u = x["link"].strip()
        t = normalize_title(x["title"])
        if u in seen_u or t in seen_t:
            continue
        seen_u.add(u); seen_t.add(t); uniq.append(x)
    items = uniq
    # 5) ì‹¤í–‰ ê°„ ì¤‘ë³µ ì œê±°(ì „ì—­)
    global RUN_SEEN_URLS, RUN_SEEN_TITLES
    tmp = []
    for x in items:
        u = x["link"].strip()
        t = normalize_title(x["title"])
        if u in RUN_SEEN_URLS or t in RUN_SEEN_TITLES:
            continue
        tmp.append(x)
    items = tmp
    # 6) ìŠ¤ì½”ì–´ë§
    items.sort(key=lambda z: score_item(z, cfg), reverse=True)
    return items

def make_batches(cfg):
    keywords = cfg.get("KEYWORDS", [])
    aliases = cfg.get("KEYWORD_ALIASES", {})
    page_size = int(cfg.get("PAGE_SIZE", 30))
    max_per = int(cfg.get("MAX_PER_KEYWORD", 10))

    picked_by_bucket = {}
    cross_seen_u, cross_seen_t = set(), set()

    for bucket in keywords:
        q_terms = [bucket] + aliases.get(bucket, [])
        q = " OR ".join(list(dict.fromkeys(q_terms)))
        raw = []
        raw += search_naver_news(q, display=page_size)
        raw += search_newsapi(q, page_size=page_size)

        filtered = filter_and_rank(raw, cfg)

        # ë²„í‚· ê°„ êµì°¨ ì¤‘ë³µ ì œê±°
        deduped = []
        for it in filtered:
            u = it["link"].strip()
            t = normalize_title(it["title"])
            if u in cross_seen_u or t in cross_seen_t:
                continue
            cross_seen_u.add(u); cross_seen_t.add(t)
            deduped.append(it)
        picked_by_bucket[bucket] = deduped[:max_per]
    return picked_by_bucket

def aggregate_top_k(batches, cfg):
    """ë²„í‚· ì „ì²´ë¥¼ í•©ì³ ìƒìœ„ Kê°œë¡œ ì •ë ¬"""
    K = int(cfg.get("MAX_OVERALL", 10))
    flat = []
    for arr in batches.values():
        for it in arr:
            flat.append((score_item(it, cfg), it))
    flat.sort(key=lambda x: x[0], reverse=True)
    top = []
    seen_u, seen_t = set(), set()
    for _, it in flat:
        u = it["link"].strip()
        t = normalize_title(it["title"])
        if u in seen_u or t in seen_t:
            continue
        seen_u.add(u); seen_t.add(t)
        top.append(it)
        if len(top) >= K:
            break
    return top

# =========================
# Telegram
# =========================
def send_telegram_message(text, disable_preview=True):
    token = os.environ.get("TELEGRAM_BOT_TOKEN", "").strip()
    chat_id = os.environ.get("TELEGRAM_CHAT_ID", "").strip()
    if not token or not chat_id:
        return False, "TELEGRAM env ë¯¸ì„¤ì •"
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": bool(disable_preview),
    }
    try:
        r = requests.post(url, data=payload, timeout=10)
        ok = r.status_code == 200 and r.json().get("ok", False)
        if not ok:
            return False, f"TG ì˜¤ë¥˜: {r.text[:200]}"
        return True, "ok"
    except Exception as e:
        return False, str(e)

def display_source(it):
    d = get_domain(it.get("link",""))
    return d or (it.get("source") or "")

def format_overall_message(items):
    if not items: return None
    lines = ["ğŸ“Œ PE ë™í–¥ ë‰´ìŠ¤ (Top 10)"]
    for it in items:
        when = it.get("pub_dt")
        ts = when.strftime("%Y-%m-%d %H:%M") if when else ""
        src = display_source(it)
        lines.append(f"â€¢ {it['title']} ({it['link']}) â€” {src} ({ts})")
    return "\n".join(lines)

def format_bucket_message(bucket, items):
    if not items: return None
    lines = [f"ğŸ“Œ PE ë™í–¥ ë‰´ìŠ¤ ({bucket})"]
    for it in items[:10]:
        when = it.get("pub_dt")
        ts = when.strftime("%Y-%m-%d %H:%M") if when else ""
        src = display_source(it)
        lines.append(f"â€¢ {it['title']} ({it['link']}) â€” {src} ({ts})")
    return "\n".join(lines)

def transmit_once(cfg, preview=False, ignore_hours=False):
    global RUN_SEEN_URLS, RUN_SEEN_TITLES, last_run_info

    # ê·¼ë¬´ì‹œê°„/íœ´ì¼ ì œí•œ (ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œë§Œ ì ìš©)
    if not ignore_hours and not within_send_window(cfg):
        last_run_info = {"ts": now_kst(), "sent": 0, "picked": 0, "note": "off_hours_or_holiday"}
        return {"picked": 0, "sent": 0, "skipped": "off_hours_or_holiday"}

    batches = make_batches(cfg)
    disable_preview = to_bool(cfg.get("TELEGRAM_DISABLE_PREVIEW", True), True)

    # ì§‘ê³„ ëª¨ë“œ ì—¬ë¶€
    aggregate = to_bool(cfg.get("AGGREGATE_MODE", True), True)

    if aggregate:
        top = aggregate_top_k(batches, cfg)
        total_picked = len(top)
        total_sent = 0
        if not preview:
            # ì „ì—­ ì¤‘ë³µ ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            for it in top:
                RUN_SEEN_URLS.add(it["link"].strip())
                RUN_SEEN_TITLES.add(normalize_title(it["title"]))
            msg = format_overall_message(top)
            if msg:
                ok, err = send_telegram_message(msg, disable_preview=disable_preview)
                if ok: total_sent = len(top)
                else: last_run_info = {"ts": now_kst(), "sent": total_sent, "picked": total_picked, "note": f"TG_FAIL:{err}"}
        last_run_info = {"ts": now_kst(), "sent": total_sent, "picked": total_picked, "note": ""}
        return {"picked": total_picked, "sent": total_sent, "skipped": None}

    # (ë¹„ì§‘ê³„ ëª¨ë“œ) ë²„í‚·ë³„ ì „ì†¡
    total_picked = sum(len(v) for v in batches.values())
    total_sent = 0
    if not preview:
        for arr in batches.values():
            for it in arr:
                RUN_SEEN_URLS.add(it["link"].strip())
                RUN_SEEN_TITLES.add(normalize_title(it["title"]))
        for bucket, arr in batches.items():
            msg = format_bucket_message(bucket, arr)
            if not msg: continue
            ok, err = send_telegram_message(msg, disable_preview=disable_preview)
            if ok: total_sent += len(arr)
            else: last_run_info = {"ts": now_kst(), "sent": total_sent, "picked": total_picked, "note": f"TG_FAIL:{err}"}

    last_run_info = {"ts": now_kst(), "sent": total_sent, "picked": total_picked, "note": ""}
    return {"picked": total_picked, "sent": total_sent, "skipped": None}

# =========================
# ìŠ¤ì¼€ì¤„ëŸ¬
# =========================
SCHED = BackgroundScheduler(timezone="Asia/Seoul")
JOB_ID = "pe_monitoring_job"
JOB_LOCK = threading.Lock()

def start_schedule(cfg):
    with JOB_LOCK:
        if SCHED.get_job(JOB_ID):
            return
        interval = int(cfg.get("TRANSMIT_INTERVAL_MIN", 60))
        SCHED.add_job(lambda: transmit_once(cfg, preview=False, ignore_hours=False),
                      "interval", minutes=interval, id=JOB_ID, max_instances=1)
        if not SCHED.running:
            SCHED.start()

def stop_schedule():
    with JOB_LOCK:
        job = SCHED.get_job(JOB_ID)
        if job:
            job.remove()

def scheduler_running():
    return SCHED.get_job(JOB_ID) is not None

# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="PE ë™í–¥ ë‰´ìŠ¤ â†’ Telegram", page_icon="ğŸ“¨", layout="wide")

cfg, cfg_meta = load_config()

with st.sidebar:
    st.markdown("### ìê²©ì¦ëª… / ì„¤ì •")
    st.caption(f"CONFIG ê²½ë¡œ:\n`{cfg_meta.get('path') or 'ë¯¸ë°œê²¬'}` / ì¡´ì¬: **{cfg_meta.get('exists', False)}**")
    st.divider()
    st.text_input("NewsAPI Key (ì„ íƒ)", value=("â—" * 8 if env_ok("NEWSAPI_KEY") else ""), disabled=True)
    st.text_input("Naver Client ID (ì„ íƒ)", value=("â—" * 8 if env_ok("NAVER_CLIENT_ID") else ""), disabled=True)
    st.text_input("Naver Client Secret (ì„ íƒ)", value=("â—" * 8 if env_ok("NAVER_CLIENT_SECRET") else ""), disabled=True)
    st.text_input("Telegram Bot Token", value=("â—" * 8 if env_ok("TELEGRAM_BOT_TOKEN") else ""), disabled=True)
    st.text_input("Telegram Chat ID", value=os.environ.get("TELEGRAM_CHAT_ID",""), disabled=True)

    st.divider()
    st.markdown("### config.json")
    if cfg:
        st.button("êµ¬ì„± ë¦¬ë¡œë“œ", on_click=lambda: load_config.clear())
        st.write("**KEYWORDS (ì½ê¸°ì „ìš©)**")
        st.code(", ".join(cfg.get("KEYWORDS", [])) or "(none)")
        st.number_input("í˜ì´ì§€ë‹¹ ìˆ˜ì§‘ ìˆ˜", 5, 100, int(cfg.get("PAGE_SIZE", 30)), 5, key="ps", disabled=True)
        st.number_input("ì „ì†¡ ê±´ìˆ˜ ì œí•œ(í‚¤ì›Œë“œë³„)", 1, 50, int(cfg.get("MAX_PER_KEYWORD", 10)), 1, key="mx", disabled=True)
        st.number_input("ì „ì†¡ ì£¼ê¸°(ë¶„)", 10, 360, int(cfg.get("TRANSMIT_INTERVAL_MIN", 60)), 10, key="iv", disabled=True)
        st.number_input("ì‹ ì„ ë„(ìµœê·¼ Nì‹œê°„)", 6, 168, int(cfg.get("RECENCY_HOURS", 48)), 6, key="rc", disabled=True)
        st.checkbox("ì—…ë¬´ì‹œê°„(08~20 KST) ë‚´ ì „ì†¡", value=to_bool(cfg.get("ONLY_WORKING_HOURS", True)), disabled=True)
        st.checkbox("ì£¼ë§ ë¯¸ì „ì†¡", value=to_bool(cfg.get("SKIP_WEEKENDS", True)), disabled=True)
        st.checkbox("ê³µíœ´ì¼ ë¯¸ì „ì†¡", value=to_bool(cfg.get("SKIP_HOLIDAYS", True)), disabled=True)
        st.checkbox("ë§í¬ í”„ë¦¬ë·° ë¹„í™œì„±í™”", value=to_bool(cfg.get("TELEGRAM_DISABLE_PREVIEW", True)), disabled=True)
        st.checkbox("ì§‘ê³„ ëª¨ë“œ(Top-K ë‹¨ì¼ ë©”ì‹œì§€)", value=to_bool(cfg.get("AGGREGATE_MODE", True)), disabled=True)
    else:
        st.error("config.jsonì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ/JSON ë¬¸ë²•ì„ í™•ì¸í•˜ì„¸ìš”.")

st.title("ğŸ“¬ PE ë™í–¥ ë‰´ìŠ¤ â†’ Telegram ìë™ ì „ì†¡")
st.caption("Streamlit + Naver/NewsAPI + Telegram + APScheduler")

col1, col2, col3, col4 = st.columns([1,1,1,1])
with col1:
    if st.button("ì§€ê¸ˆ í•œ ë²ˆ ì‹¤í–‰(ë¯¸ë¦¬ë³´ê¸°)", use_container_width=True):
        if not cfg:
            st.error("config.jsonì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            res = transmit_once(cfg, preview=True, ignore_hours=True)
            st.success(f"ì™„ë£Œ: {res['picked']}ê±´ ë¯¸ë¦¬ë³´ê¸°, 0ê±´ ì „ì†¡(ë¯¸ë¦¬ë³´ê¸°)")
with col2:
    if st.button("ì§€ê¸ˆ í•œ ë²ˆ ì „ì†¡", use_container_width=True):
        if not cfg:
            st.error("config.jsonì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            res = transmit_once(cfg, preview=False, ignore_hours=True)
            st.success(f"ì „ì†¡ ì™„ë£Œ: {res['sent']}ê±´ ì „ì†¡ / ì„ ë³„ {res['picked']}ê±´")
with col3:
    if st.button("ìŠ¤ì¼€ì¤„ ì‹œì‘", use_container_width=True):
        if not cfg:
            st.error("config.jsonì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            start_schedule(cfg)
            st.info("ìŠ¤ì¼€ì¤„ ì‹œì‘ë¨.")
with col4:
    if st.button("ìŠ¤ì¼€ì¤„ ì¤‘ì§€", use_container_width=True):
        stop_schedule()
        st.info("ìŠ¤ì¼€ì¤„ ì¤‘ì§€ë¨.")

st.divider()
st.subheader("ìƒíƒœ")
st.write(f"Scheduler ì‹¤í–‰ ì¤‘: **{scheduler_running()}**")
if last_run_info["ts"]:
    note = f" (note: {last_run_info['note']})" if last_run_info.get("note") else ""
    st.write(f"ë§ˆì§€ë§‰ ìˆ˜í–‰ ì‹œê°: **{last_run_info['ts'].strftime('%Y-%m-%d %H:%M:%S')}** / ì„ ë³„: {last_run_info['picked']} / ì „ì†¡: {last_run_info['sent']}{note}")

# ë¯¸ë¦¬ë³´ê¸°
if cfg:
    st.subheader("ë¯¸ë¦¬ë³´ê¸°")
    preview_batches = make_batches(cfg)
    if to_bool(cfg.get("AGGREGATE_MODE", True), True):
        top = aggregate_top_k(preview_batches, cfg)
        with st.expander(f"Top {int(cfg.get('MAX_OVERALL',10))} â€” {len(top)}ê±´", expanded=True):
            if not top:
                st.caption("ê²°ê³¼ ì—†ìŒ")
            for it in top:
                ts = it["pub_dt"].strftime("%Y-%m-%d %H:%M") if it.get("pub_dt") else ""
                src = display_source(it)
                st.markdown(f"- [{it['title']}]({it['link']})  \n"
                            f"  <span style='font-size:12px;color:#888'>{src} â€” {ts}</span>", unsafe_allow_html=True)
    else:
        for bucket, arr in preview_batches.items():
            with st.expander(f"{bucket} â€” {len(arr[:10])}ê±´", expanded=False):
                if not arr:
                    st.caption("ê²°ê³¼ ì—†ìŒ")
                for it in arr[:10]:
                    ts = it["pub_dt"].strftime("%Y-%m-%d %H:%M") if it.get("pub_dt") else ""
                    src = display_source(it)
                    st.markdown(f"- [{it['title']}]({it['link']})  \n"
                                f"  <span style='font-size:12px;color:#888'>{src} â€” {ts}</span>", unsafe_allow_html=True)
